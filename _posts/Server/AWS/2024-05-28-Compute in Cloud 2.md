---
title:  "[AWS]Compute in Cloud 2"
categories: AWS
tag: [AWS, Cloud]
toc: true
author_profile: false
sidebar:
    nav: "docs"
use_math: true
excerpt: AWS Cloud Practitioner Essentials
comments: true
date: 2024-05-28
toc_sticky: true
---

# 출처
앞으로 정리할 내용은 [AWS Cloud Practitioner Essentials (Korean) (Na) (한국어 강의)](https://explore.skillbuilder.aws/learn/course/13522/play/107682/aws-cloud-practitioner-essentials-korean-na-hangug-eo-gang-ui)을 기반임을 밝힙니다.   

## Amazon EC2 크기 조정
온프레미스의 경우 피크 부하가 발생하면 하드웨어가 부족해지는 현상이 발생할 수 있습니다. 그렇다고 피크 시간에 대비해 하드웨어를 크게 사용할경우, 그 외의 시간에 대해서는 낭비가 발생할 수 있습니다. 이 경우에 대해서는 온프레미스에서는 해결이 불가능합니다.   
하지만, AWS는 매일 매시간의 수요에 맞는 워크로드를 프로비저닝할 수 있도록 지원을 합니다. 즉, 회사에 필요한 ROI를 달성할 수 있도록 지원합니다. 즉, 어떤 EC2와 동일한 환경을 가지는 예비 EC2를 두어 장애가 발생했을시 대비할 수 있도록 지원해줍니다. 또한, 피크 시간에 대비해 예비 EC2를 두어 피크 시간에 해당 예비 EC2가 활성화가 되어 이를 극복합니다.    

### 확장성
확장성을 위해서는 필요한 리소스만으로 시작하고 확장 및 축소를 통해 수요 변화에 자동으로 대응하도록 아키텍처를 설계해야 합니다. 따라서 사용하는 리소스에 대해서만 비용을 지불합니다. AWS에서는 이를 위해 **Amazon EC2 Auto Scaling**을 지원합니다.   

<br>
<span style='color:blue'>Amazon EC2 Auto Scaling</span>    
피크 시간에 맞춰 EC2 인스턴스를 확장하거나 제거할 수 있습니다. 즉, 필요에 따라 인스턴스를 자동으로 조정하여 애플리케이션 가용성을 효과적으로 유지할 수 있습니다.   
Amazon EC2 Auto Scaling에는 크게 **동적 조정**과 **예측 조정**이라는 2가지 접근 방식을 사용할 수 있습니다.   
동적 조정은 수요 변화에 대응합니다.   
예측 조정은 예측된 수요에 따라 적절한 수의 Amazon EC2 인스턴스를 자동으로 예약합니다.   

### 확장 방법
증가하는 수요에 대해 **수직 확장**과 **수평 확장**으로 처리할 수 있습니다. **수직 확장**은 실행 중인 장치에 성능을 추가하는 작업이지만, 해당 요청을 더 빠르게 처리하지는 못합니다. 왜냐하면 요청을 하는 클라이언트측에 달려있기 때문입니다. **수평 확장**은 인스턴스를 추가하여 더 많은 요청에 대해 처리할 수 있도록 확장하는 방법입니다. 즉, 시스템 분리를 하며 이런 요청의 문제를 처리하기 위해 **오버프로비저닝**하는 대신 프로세스의 각 부분에 적절한 수준의 성능을 제공하도록 퍼트리는 것입니다. 그 이후, 요청의 수에 따라 Amazon EC2 Auto Scaling은 수요에 따라 인스턴스를 추가하고 필요 없는 인스턴스는 폐기합니다.   

<br>
<span style='color:blue'>Amazon EC2 Auto Scaling</span>    
Auto Scaling 그룹의 크기를 구성할 때 최소 Amazon EC2 인스턴스 수를 1로 설정할 수 있습니다. 즉, 하나 이상의 Amazon EC2 인스턴스가 항상 실행 중이어야 합니다.    
<img src="../../../assets/images/AWS Cloud Practitioner/2024-05-28-AWS Cloud Practitioner Essentials 2/Auto Scaling.png" alt="Auto Scaling" style="zoom:80%;" />    
Auto Scaling 그룹을 생성할 때 최소 Amazon EC2 인스턴스 수를 설정할 수 있습니다. **최소 용량**은 Auto Scaling 그룹을 생성한 직후 시작되는 Amazon EC2 인스턴스의 수입니다.   
최소 하나의 Amazon EC2 인스턴스가 필요하더라도 **희망 용량**을 Amazon EC2 인스턴스 2개로 설정할 수 있습니다.   
**최대 용량**설정을 통해 수요 증가에 대응하여 확장하도록 Auto Scaling 그룹을 구성하되 Amazon EC2 인스턴스 수를 최대 4개로 제한할 수 있습니다.   

## Elastic Load Balancing
EC2에 대한 규모를 조정했습니다. 하지만, **트래픽에 대한 문제**는 아직 해결하지 못했습니다. 즉, 같은 목표로 들어오는 요청에 대해 분산시켜 인스턴스에게 분배를 해줄 수 있다면, 클라이언트들의 요청을 더 빠르게 처리할 수 있을 것입니다. 이 역햘을 해주는 것이 Load Balancing입니다. LB는 요청을 받은 다음 처리할 인스턴스로 라우팅하는 애플리케이션입니다. 즉, LB는 시스템에 트래픽을 적절하게 분산하는 일일 것입니다.   
<br>
AWS에서의 Load Balancing은 Elastic Load Balancing입니다. Elastic Load Balancing은 **리전 구조**인데, 리전 구조란 개별 EC2 인스턴스가 아닌 리전 수준에서 실행되므로 사용자가 추가로 작업하지 않아도 자동으로 고가용 서비스가 된다는 것입니다. 즉, ELB는 자동 확장이며, 트래픽이 증가하면 시간당 비용 변경 없이 추가 처리량을 처리하도록 설계되었습니다.    
<br>
이번에는 Auto Scaling과 Load Balancing을 활용한 요청 처리방법의 아키텍처 구조에 대해 설명하겠습니다. 우선 요청이 확장되면 **Auto Scaling 서비스**는 Elastic Load Balancing에 인스턴스가 트래픽을 처리할 준비가 되었음을 알린 후 꺼집니다. 그럼 요청이 축소될 경우 ELB는 먼저 모든 신규 트래픽을 중지한 후 기존 요청이 완료될 때까지 기다린 다음 비웁니다.이 작업이 끝나면 Auto Scaling 엔진은 기존 고객에 대한 중단을 유발하지 않고도 인스턴스를 종료할 수 있습니다. 여기서 ELB는 외부 트래픽뿐만 아니라 내부 트래픽에서도 이용됩니다.   

## 메시징 및 대기열
보통 서비스에서 요청을 받는 프로세스와 처리하는 프로세스가 잘 맞으면 빠르게 처리할 수 있습니다. 하지만, 실제 서비스에서는 이를 맞추기가 힘듭니다. 예를 들어, 요청을 받은 프로세스가 처리하는 프로세스에게 요청을 전달해야하는데, 처리하는 프로세스가 다른 작업을 수행중이라면 요청 프로세스는 이를 대기해야합니다. 이 경우 요청 프로세스가 매우 느려지게 될 것입니다. 이를 방지하고자 **버퍼와 같은 대기열을 두어** 요청 프로세스는 받은 요청을 이에 저장한 후 처리 프로세스는 이를 보고 처리를 수행한다면 더 효율적으로 서비스가 돌아갈 수 있습니다. 이 때의 버퍼를 **메시지 큐**라고 합니다.   
여기서 메시지큐를 두지않는 아키텍처를 **밀결합된 아키텍처**라고 합니다. 밀결합된 아키텍처의 경우 처리 프로세스의 어플리케이션이 장애가 발생한다면 요청 프로세스의 어플리케이션 또한 확산되어 장애가 발생하게 됩니다.   
메시지큐를 둔 아키텍처를 **소결합된 아키텍처**라고 합니다. 이 경우, 요청 프로세스는 메시지 큐에 요청을 넣어놓기 때문에 처리 프로세스가 장애가 발생하더라도 요청 프로세스는 장애가 발생하지 않습니다. 따라서 계속하여 요청을 메시지큐에 넣어둘 수 있습니다.    
이 결과 크게 SQS(Amazon Simple Queue Service)와 SNS(Amazon Simple Notification Service)의 AWS의 서비스가 생겼습니다.   
<br>
<span style='color:blue'>Amazon Simple Notification Service(Amazon SNS)</span>    
Amazon SNS는 게시 및 구독 서비스입니다. 메시지를 서비스에 전달하는 데 사용한다는 점에서 이와 비슷하지만 알림을 최종 사용자에게도 전송할 수 있습니다. 즉, 커피숍에서 계산원이 음료를 만드는 바리스타에게 주문 사항을 전달하는 것과 비슷합니다.   
<img src="../../../assets/images/AWS Cloud Practitioner/2024-05-28-AWS Cloud Practitioner Essentials 2/SNS1.png" alt="SNS1" style="zoom:80%;" />    
단일 주제에서 업데이트 게시의 경우에는 뉴스레터를 구독하는 모든 고객은 쿠폰, 커피 상식, 신제품에 대한 업데이트를 받습니다.   
<img src="../../../assets/images/AWS Cloud Practitioner/2024-05-28-AWS Cloud Practitioner Essentials 2/SNS2.png" alt="SNS2" style="zoom:80%;" />    
여러 주제에서 업데이트 게시의 경우에는 구독자는 구독한 특정 주제에 대해서만 즉시 업데이트를 받게 됩니다.   
즉, 구독한 주제에 대해서 최종 알림을 받을 수 있습니다.   

<br>
<span style='color:blue'>Amazon Simple Queue Service(Amazon SQS)</span>    
Amazon SQS는 메시지 대기열 서비스입니다. 메시지 손실이나 다른 서비스 사용 없이 소프트웨어 구성 요소 간에 메시지를 전송, 저장, 수신할 수 있습니다.  
<img src="../../../assets/images/AWS Cloud Practitioner/2024-05-28-AWS Cloud Practitioner Essentials 2/SQS1.png" alt="SQS1" style="zoom:80%;" />    
주문 이행의 경우입니다.   
계산원은 고객의 주문을 받아 주문지에 작성 후, 바리스타에게 전달하여 음료를 만들어 고객에게 제공해줍니다. 그런데 여기서 바리스타가 다른 것을 하고 있다면, 계산원은 바리스타가 주문을 받을 때까지 대기해야합니다.   
<img src="../../../assets/images/AWS Cloud Practitioner/2024-05-28-AWS Cloud Practitioner Essentials 2/SQS2.jpg" alt="SQS2" style="zoom:80%;" />    
대기열이 있는 경우입니다.   
이전과 동일하게 계산원은 고객의 주문을 받습니다. 하지만, 계산원은 해당 주문을 주문판(대기열)에 넣습니다. 바리스타는 주문판(대기열)을 보고 해당 음료를 만들어 고객에게 제공해줍니다.    
즉, 개별 구성 요소는 이러한 분리된 접근 방식을 통해 보다 효율적이고 독립적으로 작동할 수 있습니다.   


## 추가 컴퓨팅 서비스
EC2 인스턴스는 최소한의 손실로 프로비저닝해서 AWS에서 가동하고, 실행할 수 있는 가상 머신입니다. 하지만, EC2를 사용하면 사용자가 시간에 따라 인스턴스 플릿을 직접 설정하고 관리해야 합니다. 또한 EC2를 사용할 때는 사용자가 새 소프트웨어 패키지가 출시되면 인스턴스 패치를 내가 직접 책임지고 인스턴스의 규모 조정을 설정하고, 솔루션이 가용성이 높은 방식으로 호스팅되도록 아키텍처를 설계했는지 확인해야 합니다.   
여기서 이런 관리를 편리하기 위해 AWS는 **서버리스**를 제공합니다. **서버리스**란 애플리케이션을 호스팅하는 기본 인프라나 인스턴스를 마치 서버가 없는 것처럼 관리할 필요가 없다는 뜻이니다. 즉, 필요한 프로비저닝, 규모 조정, 고가용성 및 유지 관리와 관련한 모든 기본적인 환경 관리를 AWS가 대신 처리해줍니다.    
서버리스에서는 **AWS Lambda**라는 대표적인 서비스가 있습니다. Lambda는 사용자가 코드를 Lambda 함수라는 곳에 업로드할 수 있게 도와주는 서비스입니다. 트리거를 통해 작성한 Labmda함수가 동작하며 자동으로 관리형 환경에서 동작하게됩니다. AWS Lambda는 코드를 **15분 미만**으로 실행하도록 설게되어 있어 딥러닝과 같은 곳에는 부적합하며, 백엔드 처리에 적합합니다.   

<br>
요약해보겠습니다.   
기존 방식의 애플리케이션을 호스팅하고자 하고 Linux나 Windows 같은 기본 운영 체제에 대한 완전한 액세스를 원한다면 **EC2를 사용**해야 합니다.   
단기적인 실행 함수나 서비스 지향 또는 이벤트 기반 애플리케이션을 호스팅하고자 하는데 기본 환경은 전혀 관리하고 싶지 않다면 서버리스 컴퓨팅 리소스인 **AWS Lambda**를 사용하는 것을 권장합니다.   
AWS에서 Docker 컨테이너 기반 워크로드를 실행하고 싶다면 먼저 오케스트레이션 도구인 **ECS**나 **EKS**을 사용합니다.   
여기서 도구를 선택한 후 플랫폼을 선택해야하는데, 내가 관리하는 **EC2 인스턴스**에서 실행하고 싶은지 아니면 나 대신 AWS가 모두 관리해주는 **AWS Fargate**같은 서버리스 환경에서 실행하고 싶은지에 따라 선택하면 됩니다.   

### 서버리스 컴퓨팅
① 인스턴스(가상 서버)를 프로비저닝합니다.   
② 사용자 코드를 업로드합니다.    
③ 애플리케이션이 실행되는 동안 계속해서 인스턴스를 관리합니다.   
**서버리스**는 코드가 서버에서 실행되지만 이러한 서버를 프로비저닝하거나 관리할 필요가 없다는 뜻입니다. 즉, 사용자는 어플리케이션에만 집중하면 됩니다.   
서버리스의 또다른 장점은 **자동으로 확장할 수 있는 유연성**입니다. 즉, 처리량 및 메모리와 같은 소비 단위를 수정하여 애플리케이션의 용량을 조정할 수 있습니다.   
서버리스 컴퓨팅 AWS서비스는 **AWS Lambda**입니다.   

### AWS Lambda
**AWS Lambda**는 서버를 프로비저닝하거나 관리할 필요 없이 코드를 실행할 수 있는 서비스입니다.    
사용한 컴퓨팅 시간에 대해서만 비용을 지불합니다. 코드를 실행하는 동안에만 요금이 부과됩니다.   
<img src="../../../assets/images/AWS Cloud Practitioner/2024-05-28-AWS Cloud Practitioner Essentials 2/AWSLambda.png" alt="AWSLambda" style="zoom:80%;" />    
① 코드를 Lambda에 업로드합니다.   
② AWS 서비스, 모바일 애플리케이션 또는 HTTP 엔드포인트와 같은 이벤트 소스에서 트리거되도록 코드를 설정합니다.   
③ Lambda는 트리거된 경우에만 코드를 실행합니다.   
④ 사용한 컴퓨팅 시간에 대한 요금만 지불합니다. 위의 이미지 크기 조정 예에서는 새 이미지를 업로드할 때 사용한 컴퓨팅 시간에 대해서만 비용을 지불하면 됩니다. 이미지를 업로드하면 Lambda가 트리거되어 이미지 크기 조정 기능을 위한 코드를 실행합니다.   

### 컨테이너
컨테이너는 애플리케이션의 코드와 종속성을 하나의 객체로 패키징하는 표준 방식을 제공합니다.   
컨테이너 오케스트레이션 서비스는 컨테이너식 애플리케이션을 배포, 관리, 확장하는 데 도움을 줄 수 있습니다.   
컨테이너 오케스트레이션을 제공하는 2가지 서비스인 Amazon Elastic Container Service와 Amazon Elastic Kubernetes Service이 있습니다.   

### Amazon Elastic Container Service(Amazon ECS)
AWS에서 컨테이너식 애플리케이션을 실행하고 확장할 수 있는 확장성이 뛰어난 고성능 컨테이너 관리 시스템입니다.    
Docker 컨테이너를 지원를 지원합니다.   
Docker는 애플리케이션을 신속하게 구축, 테스트, 배포할 수 있는 소프트웨어 플랫폼입니다.   

### Amazon Elastic Kubernetes Service(Amazon EKS)
AWS에서 Kubernetes를 실행하는 데 사용할 수 있는 완전관리형 서비스입니다.    
Kubernetes는 컨테이너식 애플리케이션을 대규모로 배포하고 관리하는 데 사용할 수 있는 오픈 소스 소프트웨어입니다.    

### AWS Fargate
컨테이너용 서버리스 컴퓨팅 엔진입니다. Amazon ECS와 Amazon EKS에서 작동합니다.    
AWS Fargate를 사용하는 경우 서버를 프로비저닝하거나 관리할 필요가 없습니다. AWS Fargate는 자동으로 서버 인프라를 관리합니다.   


