---
title:  "Memory"
categories: LangChain
tag: [theory, AI, LLM, LangChain, Memory]
toc: true
author_profile: false
sidebar:
    nav: "docs"
use_math: true
excerpt: Prompt
comments: true
date: 2024-10-29
toc_sticky: true
---
하기의 내용은 <a href="https://wikidocs.net/233801" target="_blank">LangChain 노트</a> 기반으로 작성했습니다.

# Memroy
Langchain에서 Memory는 사용자와의 상호작용에서 축적된 정보를 저장하고, 이를 기반으로 더 적절하고 맞춤형 응답을 제공하는 기능입니다. 이를 위해서는 Memory는 사용자 데이터 저장, 컨텍스트 유지, 개인화된 경험 등과 같은 기능을 통해 이전 사용자의 질의에 대한 정보와 답변의 정보를 담고 다음 질의에 대해 더 정확하게 제공해주도록 도와줍니다.    
langchain에서는 다양한 memory 함수들을 제공해주고 있습니다. 이번 절에서는 이에 대해 살펴보겠습니다.   


## ConversationBufferMemory
우선 대화내용을 저장하는 Memory에 대해 먼저 살펴보도록 하겠습니다.   

```python
from langchain_core.prompts import ChatPromptTemplate

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory

chatmodel = ChatOpenAI(base_url="http://127.0.0.1:1234/v1", api_key="llama-3.2-1b")

memory = ConversationBufferMemory(return_messages=True)
memory.save_context(
    inputs={
        "human": "안녕하세요, 비대면으로 은행 계좌를 개설하고 싶습니다. 어떻게 시작해야 하나요?"
    },
    outputs={
        "ai": "안녕하세요! 계좌 개설을 원하신다니 기쁩니다. 먼저, 본인 인증을 위해 신분증을 준비해 주시겠어요?"
    },
)

memory.save_context(
    inputs={"human": "네, 신분증을 준비했습니다. 이제 무엇을 해야 하나요?"},
    outputs={
        "ai": "감사합니다. 신분증 앞뒤를 명확하게 촬영하여 업로드해 주세요. 이후 본인 인증 절차를 진행하겠습니다."
    },
)

memory.save_context(
    inputs={"human": "사진을 업로드했습니다. 본인 인증은 어떻게 진행되나요?"},
    outputs={
        "ai": "업로드해 주신 사진을 확인했습니다. 이제 휴대폰을 통한 본인 인증을 진행해 주세요. 문자로 발송된 인증번호를 입력해 주시면 됩니다."
    },
)

print(memory.load_memory_variables({})["history"])
```
이제 코드에 대해 설명하겠습니다. 여기서 ConversationBufferMemory 객체를 생성해주는데 이때, return_message=True로 설정을 해주면 HumanMessage 와 AIMessage 객체를 반환해줍니다. memory 객체에 save_context라는 메서드가 존재하는데 이를 통해, 대화 기록을 저장할수 있습니다. inputs와 outputs의 인자 2개를 받는데 inputs에는 사용자의 질의, outputs에는 LLM모델의 출력을 저장합니다. 이 메서드를 통해 저장한 데이터는 history 키에 저장이 되며, load_memory_variables를 통해 저장한 데이터를 확인할 수 있습니다.    
<br>
그럼 이제 ConversationBuffer 객체에 저장한 데이터를 chain을 이용하여 LLM 모델이 어떻게 출력을 내는지 살펴보겠습니다.   

```python
from langchain_core.prompts import ChatPromptTemplate

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

chatmodel = ChatOpenAI(base_url="http://127.0.0.1:1234/v1", temperature=0, api_key="llama-3.2-1b")

conversation = ConversationChain(
    # ConversationBufferMemory를 사용합니다.
    llm=chatmodel,
    memory=ConversationBufferMemory(),
)
response = conversation.predict(
    input="안녕하세요, 비대면으로 은행 계좌를 개설하고 싶습니다. 어떻게 시작해야 하나요?"
)
print(response)
print()

# 이전 대화내용을 불렛포인트로 정리해 달라는 요청을 보냅니다.
response = conversation.predict(
    input="이전 답변을 불렛포인트 형식으로 정리하여 알려주세요."
)
print(response)
```

ConversationChain에 memory로 ConversationBufferMemroy를 사용한다고 정의를 한 후, chatmodel을 넣어준 후, 사용자 질의를 담은 predict을 해주면 응답을 볼 수 있습니다. 그 이후, 이전 내용을 기억하고 있는지 확인하기 위해, 다시한번더 predict를 해주면 이전 질의와 응답에 대한 내용을 담고있는 출력을 확인할 수 있습니다.   

## ConversationBufferWindowMemory
이전 ConversationBufferMemory의 경우 모든 버퍼를 저장하고 있으면, 과거에 대한 내용도 가지고 있기 때문에, 버퍼가 너무 커져 비효율적인 경우가 생길 수 있습니다. 이를 방지하기 위해, 최근 K개에 대한 버퍼만 가지고 있는 방법인 Sliding Window를 이용한 BufferWindowMemory를 소개하겠습니다. 이는 버퍼가 너무 커지지 않도록 가장 최근 상호작용의 슬라이딩 창을 유지하는 데 유용하며 또한 최근 정보를 담고 있어 사용자 입장에서 충분히 원하는 답변을 얻어낼 수도 있을 것 입니다.   
한 번 코드 예시를 살펴보겠습니다.   

```python
from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(k=2, return_messages=True)

memory.save_context(
    inputs={
        "human": "안녕하세요, 비대면으로 은행 계좌를 개설하고 싶습니다. 어떻게 시작해야 하나요?"
    },
    outputs={
        "ai": "안녕하세요! 계좌 개설을 원하신다니 기쁩니다. 먼저, 본인 인증을 위해 신분증을 준비해 주시겠어요?"
    },
)
memory.save_context(
    inputs={"human": "네, 신분증을 준비했습니다. 이제 무엇을 해야 하나요?"},
    outputs={
        "ai": "감사합니다. 신분증 앞뒤를 명확하게 촬영하여 업로드해 주세요. 이후 본인 인증 절차를 진행하겠습니다."
    },
)
memory.save_context(
    inputs={"human": "사진을 업로드했습니다. 본인 인증은 어떻게 진행되나요?"},
    outputs={
        "ai": "업로드해 주신 사진을 확인했습니다. 이제 휴대폰을 통한 본인 인증을 진행해 주세요. 문자로 발송된 인증번호를 입력해 주시면 됩니다."
    },
)
memory.save_context(
    inputs={"human": "인증번호를 입력했습니다. 계좌 개설은 이제 어떻게 하나요?"},
    outputs={
        "ai": "본인 인증이 완료되었습니다. 이제 원하시는 계좌 종류를 선택하고 필요한 정보를 입력해 주세요. 예금 종류, 통화 종류 등을 선택할 수 있습니다."
    },
)
memory.save_context(
    inputs={"human": "정보를 모두 입력했습니다. 다음 단계는 무엇인가요?"},
    outputs={
        "ai": "입력해 주신 정보를 확인했습니다. 계좌 개설 절차가 거의 끝났습니다. 마지막으로 이용 약관에 동의해 주시고, 계좌 개설을 최종 확인해 주세요."
    },
)
memory.save_context(
    inputs={"human": "모든 절차를 완료했습니다. 계좌가 개설된 건가요?"},
    outputs={
        "ai": "네, 계좌 개설이 완료되었습니다. 고객님의 계좌 번호와 관련 정보는 등록하신 이메일로 발송되었습니다. 추가적인 도움이 필요하시면 언제든지 문의해 주세요. 감사합니다!"
    },
)

print(memory.load_memory_variables({})['history'])
```

상기의 코드는 window size를 2로 설정했기 때문에, history를 출력해보면 "정보를 모두 입력했습니다"부터의 history를 확인해볼 수 있습니다.   

## ConversationTokenBufferMemory
ConversationTokenBufferMemory는 최근 대화의 히스토리를 버퍼를 메모리에 보관하고, 대화의 개수가 아닌 토큰 길이 를 사용하여 대화내용을 flush할 시기를 결정합니다.   

```python
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationTokenBufferMemory
from langchain.chains import ConversationChain

chatmodel = ChatOpenAI(base_url="http://127.0.0.1:1234/v1", temperature=0, api_key="llama-3.2-1b")

# 메모리 설정
memory = ConversationTokenBufferMemory(
    llm=chatmodel, max_token_limit=150, return_messages=True  # 최대 토큰 길이를 150개로 제한
)

memory.save_context(
    inputs={
        "human": "안녕하세요, 저는 최근에 여러분 회사의 공작 기계를 구매했습니다. 설치 방법을 알려주실 수 있나요?"
    },
    outputs={
        "ai": "안녕하세요! 구매해 주셔서 감사합니다. 해당 기계 모델 번호를 알려주시겠어요?"
    },
)
memory.save_context(
    inputs={"human": "네, 모델 번호는 XG-200입니다."},
    outputs={
        "ai": "감사합니다. XG-200 모델의 설치 안내를 도와드리겠습니다. 먼저, 설치할 장소의 전원 공급 상태를 확인해주세요. 기계는 220V 전원이 필요합니다."
    },
)
memory.save_context(
    inputs={"human": "전원은 확인했습니다. 다음 단계는 무엇인가요?"},
    outputs={
        "ai": "좋습니다. 다음으로, 기계를 평평하고 안정된 바닥에 배치해 주세요. 이후, 제공된 사용자 매뉴얼에 따라 케이블 연결을 진행해 주시기 바랍니다."
    },
)
memory.save_context(
    inputs={"human": "연결은 어떻게 하나요?"},
    outputs={
        "ai": "매뉴얼의 5페이지를 참조해 주세요. 케이블 연결에 관한 상세한 지침이 있습니다. 이 과정에서 어려움이 있으시면 추가적으로 도와드리겠습니다."
    },
)
memory.save_context(
    inputs={"human": "설치가 완료되면 어떻게 해야 하나요?"},
    outputs={
        "ai": "설치가 완료되면, 전원을 켜고 초기 구동 테스트를 진행해 주시기 바랍니다. 테스트 절차는 매뉴얼의 10페이지에 설명되어 있습니다. 만약 기계에 이상이 있거나 추가적인 지원이 필요하시면 언제든지 연락 주시기 바랍니다."
    },
)
memory.save_context(
    inputs={"human": "감사합니다, 도움이 많이 되었어요!"},
    outputs={
        "ai": "언제든지 도와드릴 준비가 되어 있습니다. 추가적인 질문이나 지원이 필요하시면 언제든지 문의해 주세요. 좋은 하루 되세요!"
    },
)

print(memory.load_memory_variables({})["history"])
```   
ConversationTokenBufferMemory에 max_token_limit에 지정할 최대 토큰 수를 기입해주면 그 토큰 길이만큼 내용을 저장해둘 수 있습니다.   

## ConversationEntityMemory
ConversationEntityMemory는 대화에서 특정 엔티티에 대한 주어진 사실을 기억합니다. 엔티티에 대한 정보를 LLM 사용하여 추출하고 시간이 지남에 따라 해당 엔티티에 대한 지식을 축적합니다.   

```python
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationEntityMemory
from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE

chatmodel = ChatOpenAI(base_url="http://127.0.0.1:1234/v1", temperature=0, api_key="meta-llama-3.1-8b-instruct")
conversation = ConversationChain(
    llm=chatmodel,
    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,
    memory=ConversationEntityMemory(llm=chatmodel),
)

conversation.predict(
    input="테디와 셜리는 한 회사에서 일하는 동료입니다."
    "테디는 개발자이고 셜리는 디자이너입니다. "
    "그들은최근 회사에서 일하는 것을 그만두고 자신들의 회사를 차릴 계획을 세우고 있습니다."
)

print(conversation.memory.entity_store.store)
```

Entity를 제대로 사용하기 위해 langchain에서 제공하는 ENTITY_MEMORY_CONVERSATION_TEMPLATE 프롬프트를 이용했습니다. 프롬프트의 형태는 하기와 같이 구성되어있습니다.   

```python
Context:
{entities}

Current conversation:
{history}
Last line:
Human: {input}
You:
```
Human Message의 input에 predict에 작성한 프롬프트가 들어가며, Entity는 memory.entity_store.store에 저장한 주요 Entity 정보를 확인할 수 있습니다.   

## ConversationKGMemory

ConversationKGMemory는 지식 그래프의 힘을 활용하여 정보를 저장하고 불러옵니다. 이를 통해 모델이 서로 다른 개체 간의 관계를 이해하는 데 도움을 주고, 복잡한 연결망과 역사적 맥락을 기반으로 대응하는 능력을 향상시킬 수 있습니다.   

```python
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationKGMemory

chatmodel = ChatOpenAI(base_url="http://127.0.0.1:1234/v1", temperature=0, api_key="meta-llama-3.1-8b-instruct")

memory = ConversationKGMemory(llm=chatmodel, return_messages=True)
memory.save_context(
    {"input": "이쪽은 Pangyo 에 거주중인 김셜리씨 입니다."},
    {"output": "김셜리씨는 누구시죠?"},
)
memory.save_context(
    {"input": "김셜리씨는 우리 회사의 신입 디자이너입니다."},
    {"output": "만나서 반갑습니다."},
)
print(memory.load_memory_variables({"input": "김셜리씨는 누구입니까?"}))
```
지식 그래프를 기반으로 정보를 저장하고 있다, 질의에 대해서 그래프 탐색을 통해 필요한 정보를 로드해와 답변을 해줍니다. 이 때, 정보를 나눌 때, LLM을 이용하여 정보를 그래프화 시킵니다.   
<br>
이번에는 이 지식 그래프에 chain을 적용해보겠습니다.   

```python
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationKGMemory
from langchain.prompts.prompt import PromptTemplate

chatmodel = ChatOpenAI(base_url="http://127.0.0.1:1234/v1", temperature=0, api_key="meta-llama-3.1-8b-instruct")

template = """The following is a friendly conversation between a human and an AI. 
The AI is talkative and provides lots of specific details from its context. 
If the AI does not know the answer to a question, it truthfully says it does not know. 
The AI ONLY uses information contained in the "Relevant Information" section and does not hallucinate.

Relevant Information:

{history}

Conversation:
Human: {input}
AI:"""
prompt = PromptTemplate(
    input_variables=["history", "input"], template=template)

conversation_with_kg = ConversationChain(
    llm=chatmodel, prompt=prompt, memory=ConversationKGMemory(llm=chatmodel)
)

conversation_with_kg.predict(
    input="My name is Teddy. Shirley is a coworker of mine, and she's a new designer at our company."
)

print(conversation_with_kg.memory.load_memory_variables({"input": "who is Shirley?"}))
```
ConversationChain 에 ConversationKGMemory를 메모리로 지정하였습니다. 그 후, 작성한 프롬프트에 memory 데이터가 들어가 이에 대한 정보를 기반으로 LLM이 출력을 내뱉을 수 있습니다.   

## ConversationSummaryMemory
이제 가장 복잡한 부분인 Summary 유형에 대해 살펴보겠습니다.   
ConversationSummaryMemory는 시간 경과에 따른 대화의 요약 을 생성합니다. 즉, 시간 경과에 따른 대화의 정보를 압축해준다고 생각하면됩니다. 대화가 진행되는 동안 대화를 요약하고 현재 요약을 메모리에 저장합니다. 메모리를 사용하여 지금까지의 대화 요약을 prompt/chain에 삽입합니다.    

```python
```   

memory.save_text를 통해 memory에 내용을 저장하고 있습니다. 근데, history를 확인해보면, 내용을 그대로 저장하는 것이 아닌, 대화 내용을 압축하여 저장하고 있습니다.   

## ConversationSummaryBufferMemory
ConversationSummaryBufferMemory는 상기의 ConversationSummaryMemory와 ConversationBufferMemory 두 가지를 섞은 것 입니다. 즉, 최근 대화내용의 버퍼를 메모리에 유지하되, 이전 대화내용을 완전히 플러시(flush)하지 않고 요약으로 컴파일하여 두 가지를 모두 사용합니다. 플러시할 시기는 **토큰 길이**를 기반으로 진행합니다. 즉, max_token_limit의 threshold값에 도달하면, 메모리 버퍼에 있는 내용을 요약하여 재구성 후 가지고 있게됩니다.   

```python
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationSummaryBufferMemory
from langchain.prompts.prompt import PromptTemplate

chatmodel = ChatOpenAI(base_url="http://127.0.0.1:1234/v1", temperature=0, api_key="meta-llama-3.1-8b-instruct")

memory = ConversationSummaryBufferMemory(
    llm=chatmodel,
    max_token_limit=200,  # 요약의 기준이 되는 토큰 길이를 설정합니다.
    return_messages=True,
)

memory.save_context(
    inputs={"human": "여행 중에 방문할 주요 관광지는 어디인가요?"},
    outputs={
        "ai": "이 여행에서는 파리의 에펠탑, 로마의 콜로세움, 베를린의 브란덴부르크 문, 취리히의 라이네폴 등 유럽의 유명한 관광지들을 방문합니다. 각 도시의 대표적인 명소들을 포괄적으로 경험하실 수 있습니다."
    },
)
memory.save_context(
    inputs={"human": "여행자 보험은 포함되어 있나요?"},
    outputs={
        "ai": "네, 모든 여행자에게 기본 여행자 보험을 제공합니다. 이 보험은 의료비 지원, 긴급 상황 발생 시 지원 등을 포함합니다. 추가적인 보험 보장을 원하시면 상향 조정이 가능합니다."
    },
)
memory.save_context(
    inputs={
        "human": "항공편 좌석을 비즈니스 클래스로 업그레이드할 수 있나요? 비용은 어떻게 되나요?"
    },
    outputs={
        "ai": "항공편 좌석을 비즈니스 클래스로 업그레이드하는 것이 가능합니다. 업그레이드 비용은 왕복 기준으로 약 1,200유로 추가됩니다. 비즈니스 클래스에서는 더 넓은 좌석, 우수한 기내식, 그리고 추가 수하물 허용량 등의 혜택을 제공합니다."
    },
)
memory.save_context(
    inputs={"human": "패키지에 포함된 호텔의 등급은 어떻게 되나요?"},
    outputs={
        "ai": "이 패키지에는 4성급 호텔 숙박이 포함되어 있습니다. 각 호텔은 편안함과 편의성을 제공하며, 중심지에 위치해 관광지와의 접근성이 좋습니다. 모든 호텔은 우수한 서비스와 편의 시설을 갖추고 있습니다."
    },
)
print(memory.load_memory_variables({})["history"])
```
