---
title:  "Agent3(RAG)"
categories: LangChain
tag: [theory, AI, LLM, LangChain, Agent, Tools, RAG]
toc: true
author_profile: false
sidebar:
    nav: "docs"
use_math: true
excerpt: Prompt
comments: true
date: 2024-11-05
toc_sticky: true
---
하기의 내용은 <a href="https://wikidocs.net/233801" target="_blank">LangChain 노트</a> 기반으로 작성했습니다.

## RAG
이번에는 RAG를 하되, Agent가 RAG를 수행하는 방법에 대해 알아보겠습니다. 이런 RAG를 수행하는 Agent를 보통 Agentic RAG라고부릅니다.   
가장 간편한 예시인 문서 검색을 통해 최신 정보에 접근하여 검색 결과를 가져온 후 이를 기반으로 답변을 하는 Agent를 만들어보겠습니다. 즉, 질문에 따라 문서를 검색하여 답변하거나, 인터넷 검색 도구를 활용하여 답변하는 Agent를 의미합니다.   
그럼 우선 검색을 해야하니 대표적인 검색 Tool인 **Tavily Search**를 이용해보겠습니다. 이 TavilySearch를 이용하여 검색을 한 후 최신 정보에 접근하여 검색 결과를 가지고 답변을 생성할 수 있습니다.   

<br>
Tavily Search는 <a href="https://drepion43.github.io/langchain/agent_tool/" target="_blank">Agent1(Tools)</a>에서 확인해보실 수 있습니다.    

### Retrival
상기의 url을 확인하면 Tavily Search의 Tool을 만드실 수 있으실겁니다. 그럼 이번에는 Agentic RAG의 메인인 Retrival Tool을 만들어보겠습니다.    
기존 Retrieval를 만들 때와 동일한 방법으로 하기와 같이 우선 작성을 했습니다.   

```python
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

embedding = OpenAIEmbeddings(base_url='http://localhost:1234/v1', check_embedding_ctx_length=False,
                             api_key='text-embedding-multilingual-e5-large-instruct')

loader = PyPDFLoader("../data/SPRI_AI_Brief_2023년12월호_F.pdf")

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
split_docs = loader.load_and_split(text_splitter)
vector = FAISS.from_documents(split_docs, embedding)
# retriever todtjd
retriever = vector.as_retriever()

print(retriever.invoke("삼성전자가 개발한 생성형 AI 관련 내용을 문서에서 찾아줘"))
```

그 다음 langchain에서는 retrieval을 위한 tool 메서드인 create_retriever_tool을 지원해주는데, 이를 이용해 retriever를 tool로 변환시켰습니다.   

```python
from langchain.tools.retriever import create_retriever_tool

retriever_tool = create_retriever_tool(
    retriever,
    name="pdf_search",  # 도구의 이름을 입력합니다.
    description="use this tool to search information from the PDF document",  # 도구에 대한 설명을 자세히 기입해야 합니다!!
)
```

그럼 이제 Tavily Search Tool과 Retrieval Tool을 모두 정의했으니, 이 Tool들을 이용하는 Agent를 생성해보겠습니다.   
```python
from langchain_core.prompts import ChatPromptTemplate

# LLM 정의
llm = ChatOpenAI(base_url="http://127.0.0.1:1234/v1", temperature=0, api_key="meta-llama-3.1-8b-instruct")

# Prompt 정의
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant. "
            "Make sure to use the `pdf_search` tool for searching information from the PDF document. "
            "If you can't find the information from the PDF document, use the `search` tool for searching information from the web.",
        ),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),
    ]
)

from langchain.agents import create_tool_calling_agent

# tool calling agent 생성
agent = create_tool_calling_agent(llm, tools, prompt)

from langchain.agents import AgentExecutor
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)

from langchain_teddynote.messages import AgentStreamParser

# 각 단계별 출력을 위한 파서 생성
agent_stream_parser = AgentStreamParser()

# 질의에 대한 답변을 스트리밍으로 출력 요청
result = agent_executor.stream(
    {"input": "2024년 프로야구 플레이오프 진출한 5개 팀을 검색하여 알려주세요."}
)

for step in result:
    # 중간 단계를 parser 를 사용하여 단계별로 출력
    agent_stream_parser.process_agent_steps(step)
    
    
# 질의에 대한 답변을 스트리밍으로 출력 요청
result = agent_executor.stream(
    {"input": "삼성전자가 자체 개발한 생성형 AI 관련된 정보를 문서에서 찾아주세요."}
)

for step in result:
    # 중간 단계를 parser 를 사용하여 단계별로 출력
    agent_stream_parser.process_agent_steps(step)
```

우선 "2024년 프로야구 플레이오프 진출한 5개 팀을 검색하여 알려주세요."의 쿼리에 대해서는 pdf파일에 없기 때문에, tavily search를 통해 갖고온 정보를 기반으로 LLM이 답변을 하게 됩니다. 그 후, "삼성전자가 자체 개발한 생성형 AI 관련된 정보를 문서에서 찾아주세요."은 pdf에 존재하는 내용이므로, RAG를 통해 잘 답변을 하는 것을 실습을 통해 확인하실 수 있으실겁니다.    
<br>
그럼 저번과 마찬가지로 history를 기억하는 Agent도 추가적으로 만들어보겠습니다.    

```python
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# session_id 를 저장할 딕셔너리 생성
store = {}


# session_id 를 기반으로 세션 기록을 가져오는 함수
def get_session_history(session_ids):
    if session_ids not in store:  # session_id 가 store에 없는 경우
        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장
        store[session_ids] = ChatMessageHistory()
    return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환


# 채팅 메시지 기록이 추가된 에이전트를 생성합니다.
agent_with_chat_history = RunnableWithMessageHistory(
    agent_executor,
    # 대화 session_id
    get_session_history,
    # 프롬프트의 질문이 입력되는 key: "input"
    input_messages_key="input",
    # 프롬프트의 메시지가 입력되는 key: "chat_history"
    history_messages_key="chat_history",
)

from langchain_teddynote.messages import AgentStreamParser

# 각 단계별 출력을 위한 파서 생성
agent_stream_parser = AgentStreamParser()

# 질의에 대한 답변을 스트리밍으로 출력 요청
response = agent_with_chat_history.stream(
    {"input": "삼성전자가 개발한 생성형 AI 관련된 정보를 문서에서 찾아주세요."},
    # session_id 설정
    config={"configurable": {"session_id": "abc123"}},
)

# 출력 확인
for step in response:
    agent_stream_parser.process_agent_steps(step)
    
    
response = agent_with_chat_history.stream(
    {"input": "이전의 답변을 영어로 번역해 주세요."},
    # session_id 설정
    config={"configurable": {"session_id": "abc123"}},
)

# 출력 확인
for step in response:
    agent_stream_parser.process_agent_steps(step)
```