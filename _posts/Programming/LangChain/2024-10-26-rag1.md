---
title:  "RAG1"
categories: LangChain
tag: [theory, AI, LLM, LangChain, RAG]
toc: true
author_profile: false
sidebar:
    nav: "docs"
use_math: true
excerpt: Prompt
comments: true
date: 2024-10-26
toc_sticky: true
---
하기의 내용은 <a href="https://wikidocs.net/book/14473" target="_blank">LangChain 입문부터 응용까지</a> 기반으로 작성했습니다.

# RAG
RAG는 Retrieval-Augmented Generation의 약어로 기존의 LLM을 확장하여, 주어진 컨텍스트나 질문에 대해 더 정확하고 많은 정보를 제공하게 해주는 기법입니다. 여기서 Retrieval는 검색이라는 의미이며, 모델이 학습할 때, 사용하지 않은 외부 데이터를 실시간으로 검색하는 역할을 하며, 이를 기반으로 Generation하여 답변해주는 것을 의미합니다. RAG는 특히 LLM모델에서 가장 중요한 **Hallucination(환각) 현상을 줄여주는 역할**을 하며, 최신 정보를 더 잘 반영하게 해줍니다.   

## RAG 절차
RAG는 크게 5가지의 절차로 이루어집니다.    
1. 데이터 로드
    - RAG에 사용할 데이터를 불러오는 단계입니다. 이런 데이터는 다양한 방법으로 불러올 수 있습니다. 웹 크롤링, 공개 데이터셋 등과 같은 데이터가 될 수도 있습니다. 하기에는 langchain에서 제공하는 WebBaseLoader라는 모듈인데, 이 모듈은 지정한 url의 웹페이지에 존재하는 데이터를 가져오는 것을 보여줍니다.   


2. 텍스트 분할
    - 불러온 데이터를 그대로, 모델의 프롬프트에 넣어주는 것은 토큰를 고려하여 불가능한 경우가 존재합니다. 또한, 모든 데이터들의 의미가 이어지는 경우도 항상 존재하는 것은 아니기 때문에, 의미나 문장 단위로 잘 구분될 수 있도록, chunk 단위로 분할해주는 역할을 해주어야 검색 효율이 크게 증가합니다. langchain에서는 text_splitter 라는 모듈에 다양한 텍스트 분할 기능을 제공해줍니다. 이에 대해서는 하기에 좀 더 자세히 살펴보겠습니다.

3. 인덱싱
    - 인덱싱에서는 분할된 텍스트를 검색가능한 형태로 만드는 단계입니다. 인덱싱은 검색 시간을 단축시키고, 정확도 또한 높여줍니다. langchain에서는 이런 인덱싱을 위해 다양한 vector db를 제공합니다. 크게 FAISS, Chroma 등이 존재합니다. 이 vector DB에서는 토큰들을 embedding 형태로 가지고 있어, 각 embedding들과의 유사도 검사를 통해 높은 정확도의 데이터를 추출할 수 있습니다. 

4. 검색(Retrieval)
    - 사용자의 질문이나 주어진 컨텍스트에 가장 관련된 정보를 찾아내는 과정입니다.사용자의 입력을 바탕으로 쿼리를 생성하고, 인덱싱된 데이터에서 가장 관련성 높은 정보를 검색합니다.

5. 생성(Generation)
    - 검색된 정보를 바탕으로 사용자의 질문에 답변을 생성하는 최종 단계입니다. 모델은 사전 학습된 지식과 검색 결과를 결합하여 주어진 질문에 가장 적절한 답변을 생성합니다.

## DataLoader
LangChain에서 Document Loader는 다양한 소스에서 문서를 불러오고 처리하는 과정을 담당합니다. 

### WebBaseLoader
WebBaseLoader는 웹 페이지에서 문서를 로드하는 역할을 합니다. 특정 웹 페이지의 내용을 로드하고 파싱하기 위해 설계된 클래스입니다. WebBaseLoader의 들어가는 인자들은 하기와 같습니다.   
```python
def __init__(
    self,
    web_path: Union[str, Sequence[str]] = "",
    header_template: Optional[dict] = None,
    verify_ssl: bool = True,
    proxies: Optional[dict] = None,
    continue_on_failure: bool = False,
    autoset_encoding: bool = True,
    encoding: Optional[str] = None,
    web_paths: Sequence[str] = (),
    requests_per_second: int = 2,
    default_parser: str = "html.parser",
    requests_kwargs: Optional[Dict[str, Any]] = None,
    raise_for_status: bool = False,
    bs_get_text_kwargs: Optional[Dict[str, Any]] = None,
    bs_kwargs: Optional[Dict[str, Any]] = None,
    session: Any = None,
    *,
    show_progress: bool = True,
) -> None:
"""Initialize loader.

Args:
    web_paths: Web paths to load from.
    requests_per_second: Max number of concurrent requests to make.
    default_parser: Default parser to use for BeautifulSoup.
    requests_kwargs: kwargs for requests
    raise_for_status: Raise an exception if http status code denotes an error.
    bs_get_text_kwargs: kwargs for beatifulsoup4 get_text
    bs_kwargs: kwargs for beatifulsoup4 web page parsing
    show_progress: Show progress bar when loading pages.
"""
```

인자들 중 bs로 시작하는 것을 볼 수 있는데, 이는 BeautifulSoup4를 의미합니다. bs_get_text_kwargs나 bs_kwargs에 값을 넣음으로써 원하느 형태의 데이터만 불러올 수도 있습니다.    
하기의 간단한 예시를 살펴보겠습니다.   

```python
import bs4
from langchain_community.document_loaders import WebBaseLoader

# 여러 개의 url 지정 가능
url1 = "https://blog.langchain.dev/customers-replit/"
url2 = "https://blog.langchain.dev/langgraph-v0-2/"

loader = WebBaseLoader(
    web_paths=(url1, url2),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("article-header", "article-content")
        )
    ),
)
docs = loader.load()
print(len(docs))
```

상기의 코드에서는 url1, url2에서 article-header와 article-content에 해당하는 데이터만 불러오는 역할을 합니다. 