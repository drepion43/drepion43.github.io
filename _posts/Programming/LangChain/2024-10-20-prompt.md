---
title:  "Prompt"
categories: LangChain
tag: [theory, AI, LLM, LangChain, prompt]
toc: true
author_profile: false
sidebar:
    nav: "docs"
use_math: true
excerpt: Prompt
comments: true
date: 2024-10-20
toc_sticky: true
---
하기의 내용은 <a href="https://wikidocs.net/book/14473" target="_blank">LangChain 입문부터 응용까지</a> 기반으로 작성했습니다.

## Prompt
이어서 이번에는 Prompt가 무엇인지에 대해 알아보겠습니다. Prompt는 **사용자와 언어 모델 간의 대화에서 질문이나 요청의 형태로 제시되는 입력문**입니다. 따라서 Prompt는 모델이 어떤 유형의 응답을 제공할지 결정하는 데 중요한 역할을 합니다. 즉, 모델이 최대한 정확한 답변을 할 수 있도록 효과적인 Prompt를 작성하는 것이 무엇보다 중요합니다.   
<br>
그럼 Prompt의 작성 규칙에 대해 알아보겠습니다.   
1. 명확성과 구체성
    - 질문은 명확하고 구체적이어야 합니다. 모호한 질문은 LLM 모델의 혼란을 초래할 수 있기 때문입니다.
2. 배경 정보를 포함
    - 모델이 문맥을 이해할 수 있도록 필요한 배경 정보를 제공하는 것이 좋습니다. 이는 환각 현상(hallucination)이 발생할 위험을 낮추고, 관련성 높은 응답을 생성하는 데 도움을 줍니다.
3. 간결함
    - 핵심 정보에 초점을 맞추고, 불필요한 정보는 배제합니다. 프롬프트가 길어지면 모델이 덜 중요한 부분에 집중하거나 상당한 영향을 받는 문제가 발생할 수 있습니다.
4. 열린 질문 사용
    - 열린 질문을 통해 모델이 자세하고 풍부한 답변을 제공하도록 유도합니다. 단순한 '예' 또는 '아니오'로 대답할 수 있는 질문보다는 더 많은 정보를 제공하는 질문이 좋습니다.
5. 명확한 목표 설정
    - 얻고자 하는 정보나 결과의 유형을 정확하게 정의합니다. 이는 모델이 명확한 지침에 따라 응답을 생성하도록 돕습니다.
6. 언어와 문체
    - 대화의 맥락에 적합한 언어와 문체를 선택합니다. 이는 모델이 상황에 맞는 표현을 선택하는데 도움이 됩니다.


<br>

### PromptTemplate
그럼 Prompt를 작성할 때 구조인 Template에 대해 알아보겠습니다. Prompt를 구성할 때 "지시", "예시", "맥락", "질문" 과 같은 다양한 구성요소들을 조합할 수 있습니다.   
<br>
그럼 우선 가장 간단한 **문자열 Template**에 대해 알아보겠습니다.    

```python
from langchain_core.prompts import PromptTemplate

# 'name'과 'age'라는 두 개의 변수를 사용하는 프롬프트 템플릿을 정의
template_text = "안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다."

# PromptTemplate 인스턴스를 생성
prompt_template = PromptTemplate.from_template(template_text)

# 템플릿에 값을 채워서 프롬프트를 완성
filled_prompt = prompt_template.format(name="홍길동", age=30)
print(filled_prompt)
```

template_text에 들어있는 name과 age 변수에 입력을 받으면 채워지게 되는 구조입니다. 이 Template는 여러개를 연결시킬 수도 있습니다. 하기에 예시를 살펴보겠습니다.   

```python
from langchain_core.prompts import PromptTemplate

# 'name'과 'age'라는 두 개의 변수를 사용하는 프롬프트 템플릿을 정의
template_text = "안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다."

# PromptTemplate 인스턴스를 생성
prompt_template = PromptTemplate.from_template(template_text)

combined_prompt = (
              prompt_template
              + PromptTemplate.from_template("\n\n아버지를 아버지라 부를 수 없습니다.")
              + "\n\n{language}로 번역해주세요."
)

# 템플릿에 값을 채워서 프롬프트를 완성
combined_prompt.format(name="홍길동", age=30, language="영어")
print(combined_prompt)
```

상기와 같이 그냥 string이기 때문에, + 연산으로 문자열을 합쳐주듯 PromptTemplate의 인스턴스들을 서로 연결시켜줄 수 있습니다.   

### ChatPromptTemplate
기존 문자열 Template의 단일 메세지에 대해서만 가능하며 직접 연결을 수행해줘야했습니다. 하지만, 이는 대화형 상황에서는 약간 버거운감이 있습니다. 따라서 이런 대화형을 위한 ChatPromptTemplate이라는 것이 존재합니다. 입력은 여러 메시지를 원소로 갖는 리스트로 구성되며, 각 메시지는 역할(role)과 내용(content)으로 구성됩니다.   
**Message**는 SystemMessage(시스템의 기능을 설명), HumanMessage(사용자의 질문), AIMessage(AI 모델의 응답), FunctionMessage(특정 함수 호출의 결과), ToolMessage(도구 호출의 결과)가 존재합니다.   
<br>
ChatPromptTemplate는 크게 튜플형식과 MessagePromptTemplate를 이용한 형식으로 구현할 수 있습니다. 하기는 두가지 형식에 대한 예시입니다.    

```python
# Tuple형식
from langchain_core.prompts import ChatPromptTemplate

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "이 시스템은 천문학 질문에 답변할 수 있습니다."),
    ("user", "{user_input}"),
])

messages = chat_prompt.format_messages(user_input="태양계에서 가장 큰 행성은 무엇인가요?")
print(messages)

# MessageTemplate
from langchain_core.prompts import SystemMessagePromptTemplate,  HumanMessagePromptTemplate

chat_prompt = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template("이 시스템은 천문학 질문에 답변할 수 있습니다."),
        HumanMessagePromptTemplate.from_template("{user_input}"),
    ]
)

messages = chat_prompt.format_messages(user_input="태양계에서 가장 큰 행성은 무엇인가요?")
print(messages)
```

### Few-Shot Template
Few-Shot, One-shot, Zero-Shot 등 딥러닝에서 자주 사용되는 용어입니다. 그냥 직역그대로 Few-Shot은 약간의 학습 데이터, One-shot 1개의 학습데이터, Zero-Shot은 학습데이터가 없이 주어졌을 때, 모델이 잘 예측하게 해주는 방법을 의미합니다. 이 개념 그대로 Few-Shot Template은 몇개의 주어진 예제들을 참고하여 모델이 더 정확하게 답변할 수 있도록 해주는 기법을 의미합니다.    
그럼 Few-Shot을 하기 위해서 예제를 주는 기본 Template을 몇개 만들어 모델에 Input으로 넣어주면 됩니다.    

```python
from langchain_core.prompts import PromptTemplate
from langchain_openai import OpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import FewShotPromptTemplate

llm = OpenAI(base_url="http://127.0.0.1:1234/v1", api_key="meta-llama-3.1-8b-instruct")

example_prompt = PromptTemplate.from_template("질문: {question}\n{answer}")

# 답변 예시
examples = [
    {
        "question": "지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?",
        "answer": "지구 대기의 약 78%를 차지하는 질소입니다."
    },
    {
        "question": "광합성에 필요한 주요 요소들은 무엇인가요?",
        "answer": "광합성에 필요한 주요 요소는 빛, 이산화탄소, 물입니다."
    },
    {
        "question": "피타고라스 정리를 설명해주세요.",
        "answer": "피타고라스 정리는 직각삼각형에서 빗변의 제곱이 다른 두 변의 제곱의 합과 같다는 것입니다."
    },
    {
        "question": "지구의 자전 주기는 얼마인가요?",
        "answer": "지구의 자전 주기는 약 24시간(정확히는 23시간 56분 4초)입니다."
    },
    {
        "question": "DNA의 기본 구조를 간단히 설명해주세요.",
        "answer": "DNA는 두 개의 폴리뉴클레오티드 사슬이 이중 나선 구조를 이루고 있습니다."
    },
    {
        "question": "원주율(π)의 정의는 무엇인가요?",
        "answer": "원주율(π)은 원의 지름에 대한 원의 둘레의 비율입니다."
    }
]


# FewShotPromptTemplate을 생성합니다.
prompt = FewShotPromptTemplate(
    examples=examples,              # 사용할 예제들
    example_prompt=example_prompt,  # 예제 포맷팅에 사용할 템플릿
    suffix="질문: {input}",          # 예제 뒤에 추가될 접미사
    input_variables=["input"],      # 입력 변수 지정
)

# 새로운 질문에 대한 프롬프트를 생성하고 출력합니다.
print(prompt.invoke({"input": "화성의 표면이 붉은 이유는 무엇인가요?"}).to_string())
```

상기의 예제 코드에서 보이는 example 리스트는 모델이 저런식으로 답변하도록 예제를 던져주어 유도하는 역할을 합니다.    

<br>
<br>

그럼 이번에는 주어진 예시들에서 의미적으로 가장 유사한 예제를 기반으로 답변하는 **SemanticSimilarityExampleSelector**에 대해 알아보겠습니다. 이는 입력 질문과 가장 유사한 예제를 선택 후, 답변을 함으로 더 관련성 높은 컨텍스트를 제공하여 모델의 응답 품질을 향상시킬 수 있습니다. 의미적으로 유사한 예제를 찾기 때문에 **Embedding 모델**을 입력으로 받습니다.    

```python
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAI
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
llm = OpenAI(base_url="http://127.0.0.1:1234/v1", api_key="meta-llama-3.1-8b-instruct")

embedding = OpenAIEmbeddings(base_url='http://localhost:1234/v1', check_embedding_ctx_length=False,
                             api_key='multilingual-e5-large-instruct')

# 답변 예시
examples = [
    {
        "question": "지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?",
        "answer": "지구 대기의 약 78%를 차지하는 질소입니다."
    },
    {
        "question": "광합성에 필요한 주요 요소들은 무엇인가요?",
        "answer": "광합성에 필요한 주요 요소는 빛, 이산화탄소, 물입니다."
    },
    {
        "question": "피타고라스 정리를 설명해주세요.",
        "answer": "피타고라스 정리는 직각삼각형에서 빗변의 제곱이 다른 두 변의 제곱의 합과 같다는 것입니다."
    },
    {
        "question": "지구의 자전 주기는 얼마인가요?",
        "answer": "지구의 자전 주기는 약 24시간(정확히는 23시간 56분 4초)입니다."
    },
    {
        "question": "DNA의 기본 구조를 간단히 설명해주세요.",
        "answer": "DNA는 두 개의 폴리뉴클레오티드 사슬이 이중 나선 구조를 이루고 있습니다."
    },
    {
        "question": "원주율(π)의 정의는 무엇인가요?",
        "answer": "원주율(π)은 원의 지름에 대한 원의 둘레의 비율입니다."
    }
]



example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples,            # 사용할 예제들
    OpenAIEmbeddings(),  # 임베딩 모델
    FAISS,              # 벡터 저장소
    k=1,                 # 선택할 예제 수
)

# 새로운 질문에 대해 가장 유사한 예제를 선택합니다.
question = "화성의 표면이 붉은 이유는 무엇인가요?"
selected_examples = example_selector.select_examples({"question": question})
print(f"입력과 가장 유사한 예제: {question}")
for example in selected_examples:
    print("\n")
    for k, v in example.items():
        print(f"{k}: {v}")
```