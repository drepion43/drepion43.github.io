---
title:  "Model"
categories: LangChain
tag: [theory, AI, LLM, LangChain, Model, ChatMdoel]
toc: true
author_profile: false
sidebar:
    nav: "docs"
use_math: true
excerpt: Prompt
comments: true
date: 2024-10-25
toc_sticky: true
---
하기의 내용은 <a href="https://wikidocs.net/book/14473" target="_blank">LangChain 입문부터 응용까지</a> 기반으로 작성했습니다.

# Model
Langchain에서는 크게 **LLM** 모델과 **ChatModel** 2개를 지원합니다. 얼핏보면 이 2개가 뭐가 다른지 잘 모를 수 있습니다. 하지만, 이 두 모델은 각기 다른 특성과 용도를 가지고 있으며, 사용자의 요구사항에 맞춰 적절하게 사용해줘야합니다. 일반적으로 **LLM**은 주로 단일 요청에 대한 복잡한 출력을 생성하는 데 적합하지만, **ChatModel**은 사용자와의 상호작용을 통한 연속적인 대화 관리에 더 적합합니다.    

## LLM
Langchain에서는 LLM 업체와의 상호작용을 위한 표준 인턴페이스를 제공합니다. 이는 LangChain에서 직접 LLM을 제공한느 것이 아닌, OpenAI, Cohere, Hugging Face 등과 같은 다양한 LLM 제공 업체로부터 모델을 사용할 수 있게 하는 플랫폼 역할을 한다는 것입니다. 필자는 이번에 LM Studio를 이용한 Local 환경에서의 LLM 모델을 다루겠습니다.   
```python
from langchain_openai import OpenAI

llm = OpenAI(base_url="http://127.0.0.1:1234/v1", api_key="meta-llama-3.1-8b-instruct")

print(llm.invoke("Apple이라는 회사에 대해 간략하게 설명해줘."))
```

## ChatModel
기존 LLM과 다르게 ChatModel은 대화형 메시지를 입력으로 사용하고 대화형 메시지를 출력으로 반환하는 특수화된 LLM 모델입니다. 일반 텍스트를 사용하는 대신 대화의 맥락을 포함한 메시지를 처리하며, 이를 통해 보다 더 자연스러운 대화를 가능하게 합니다.   
똑같이 LLM 모델을 쓰는것과 비슷하며 추가적으로 다양한 작동 모드 지원을 지원합니다. sync, async, batching, streaming 모드 등을 지원합니다.   

```python
from langchain_core.prompts import ChatPromptTemplate

from langchain_openai import ChatOpenAI

chatmodel = ChatOpenAI(base_url="http://127.0.0.1:1234/v1", api_key="meta-llama-3.1-8b-instruct")

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "이 시스템은 여행 전문가입니다."),
    ("user", "{user_input}"),
])

chain = chat_prompt | chatmodel
chain.invoke({"user_input": "안녕하세요? 한국의 대표적인 관광지 3군데를 추천해주세요."})
```

## 파라미터
Langchain을 통해 모델을 로드할 때, 모델의 파라미터값을 줄 수 있습니다. 크게 모델을 호출할 때, 같이 파라미터를 넘겨주는 방법과 모델을 호출 후, 파라미터를 변경하는 2가지 방법이 있습니다.    
<br>
우선 모델을 호출할 때, 파라미터를 어떻게 넘겨주는지에 대해 알아보겠습니다. 이 방법은 모델 인스턴스 생성 시 또는 모델을 호출하는 시점에 파라미터를 인수로 제공함으로써, 해당 호출에 대해서만 파라미터 설정을 적용합니다. 이는 특정한 호출에 대한 파라미터를 사용자가 직접 세밀하게 조정할 수 있다는 장점이 있습니다.  

```python
from langchain_openai import ChatOpenAI
params = {
    "temperature": 0.7,         # 생성된 텍스트의 다양성 조정
    "max_tokens": 100,          # 생성할 최대 토큰 수    
}

kwargs = {
    "frequency_penalty": 0.5,   # 이미 등장한 단어의 재등장 확률
    "presence_penalty": 0.5,    # 새로운 단어의 도입을 장려
    "stop": ["\n"]              # 정지 시퀀스 설정

}

chatmodel = ChatOpenAI(base_url="http://127.0.0.1:1234/v1", api_key="meta-llama-3.1-8b-instruct", model_kwargs = kwargs)

question = "태양계에서 가장 큰 행성은 무엇인가요?"
response = chatmodel.invoke(input=question)

# 전체 응답 출력
print(response)
```
상기와 같은 방식으로 ChatOpenAI를 호출할 때, 파라미터를 지정해, model_kwargs에 넘겨주면 됩니다.   

<br>
그럼 이제 모델을 호출 후, 파라미터를 변경하는 방법에 대해 알아보겠습니다. bind 메소드를 사용하여 모델 인스턴스에 파라미터를 추가로 제공할 수 있습니다. 보통 특수한 상황에서 일부 파라미터를 다르게 적용하고 싶을 때 사용합니다.    

```python
from langchain_core.prompts import ChatPromptTemplate

from langchain_openai import ChatOpenAI
prompt = ChatPromptTemplate.from_messages([
    ("system", "이 시스템은 천문학 질문에 답변할 수 있습니다."),
    ("user", "{user_input}"),
])

chatmodel = ChatOpenAI(base_url="http://127.0.0.1:1234/v1", api_key="meta-llama-3.1-8b-instruct")

messages = prompt.format_messages(user_input="태양계에서 가장 큰 행성은 무엇인가요?")

before_answer = chatmodel.invoke(messages)

# # binding 이전 출력
print(before_answer)

# 모델 호출 시 추가적인 인수를 전달하기 위해 bind 메서드 사용 (응답의 최대 길이를 10 토큰으로 제한)
chain = prompt | chatmodel.bind(max_tokens=10)

after_answer = chain.invoke({"user_input": "태양계에서 가장 큰 행성은 무엇인가요?"})

# binding 이후 출력
print(after_answer)
```