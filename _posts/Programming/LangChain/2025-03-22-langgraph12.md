---
title:  "LangGraph(Adaptive RAG)"
categories: LangChain
tag: [theory, AI, LLM, LangChain, LangGraph, RAG]
toc: true
author_profile: false
sidebar:
    nav: "docs"
use_math: true
excerpt: Agent
comments: true
date: 2025-03-22
toc_sticky: true
---
í•˜ê¸°ì˜ ë‚´ìš©ì€ <a href="https://wikidocs.net/233801" target="_blank">LangChain ë…¸íŠ¸</a> ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.

# Adaptive RAG
Adaptive RAGëŠ” ìµœê·¼ì— <a href="https://arxiv.org/abs/2403.14403" target="_blank">ë…¼ë¬¸: Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity</a>ì—ì„œ ì œì•ˆëœ RAGë°©ë²•ë¡ ì…ë‹ˆë‹¤. **ì¿¼ë¦¬ ë¶„ì„**ê³¼ **Active/Self-Correcting RAG(ëŠ¥ë™ì /ìê¸° ë¶„ì„ RAG)**ë¥¼ ê²°í•©í•˜ì—¬ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì „ëµë°©ë²•ì…ë‹ˆë‹¤. ì´ë²ˆ ì ˆì—ì„œëŠ” ì¶”ê°€ì ìœ¼ë¡œ ìµœì‹  ì´ë²¤íŠ¸ê´€ë ¨ ì§ˆì˜ëŠ” ì›¹ ê²€ìƒ‰ì„ í†µí•´ ìˆ˜í–‰í•˜ê³ , ì¸ë±ìŠ¤ê´€ë ¨ ì§ˆì˜ëŠ” self-Correcting RAGë¥¼ ì´ìš©í•˜ì—¬ ìˆ˜í–‰í•˜ëŠ” ì „ëµìœ¼ë¡œ ì‹¤ìŠµì„ í•´ë³´ê² ìŠµë‹ˆë‹¤. ìš°ì„  teddynoteë‹˜ê»˜ì„œ êµ¬í˜„í•œ ë¼ìš°íŒ… ë°©ë²•ì„ í•˜ê¸°ì—ì„œ ê°™ì´ í™•ì¸í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.   
<div style="text-align : center;">
<img src="../../../assets/images/LangChain/2025-03-22-langgraph12/teddynote_adaptive_langgraph.png" alt="teddynote_adaptive_langgraph" style="zoom:150%;" />    
</div>    
Routingì—ì„œ Retrieveë¥¼ í• ì§€, Web Searchë¥¼ í• ì§€ ê²°ì •í•˜ì—¬ ë¼ìš°íŒ…ì„ ìˆ˜í–‰í•´ì¤ë‹ˆë‹¤. ë˜í•œ, Retrieve ìª½ì—ì„œëŠ” self-correctingì„ í†µí•´, retrieveí•œ ê²°ê³¼ì— ëŒ€í•´ relevence checkë¡œ ë§Œì•½ ë¶€í•©í•˜ì§€ ì•Šë‹¤ë©´, Query rewriteí•œ í›„, ë‹¤ì‹œ retrieveë¥¼ ìˆ˜í–‰í•˜ê²Œ í•´ì¤ë‹ˆë‹¤. ê·¸ëŸ¼ ì´ì œ ì§ì ‘ êµ¬í˜„ì„ í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.    

## Query Routing
Adaptive-RAGì—ì„œëŠ” **íš¨ìœ¨ì ì¸ ì •ë³´ ìƒì„±ê³¼ ê²€ìƒ‰**ì„ ìœ„í•´ **Query Routing**ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ê·¸ëŸ¼ Query Routingì´ë€ **ì‚¬ìš©ìì˜ Queryë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì •ë³´ ì†ŒìŠ¤ë¡œ Routingì„ í•´ì£¼ë©°, Queryì˜ ëª©ì ì— ë§ëŠ” ìµœì ì˜ ê²€ìƒ‰ ê²½ë¡œë¥¼ ì„¤ì •**í•˜ë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤. ì¦‰, ìš°ë¦¬ê°€ ìˆ˜í–‰í•˜ëŠ” taskì—ì„œëŠ” Routingì´ ì›¹ ê²€ìƒ‰ê³¼ retrieveê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆì˜ë¥¼ ë³´ê³  ì–´ë–¤ Nodeë¡œ ë³´ë‚¼ì§€ë¥¼ ê²°ì •í•´ì£¼ëŠ” ì—­í• ì„ í•˜ëŠ”ê²Œ Query Routingì´ë¼ê³  ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤. í•˜ê¸°ì— ì½”ë“œë¥¼ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•˜ê² ìŠµë‹ˆë‹¤.    
```python
# ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë°ì´í„° ì†ŒìŠ¤ë¡œ ë¼ìš°íŒ…í•˜ëŠ” ë°ì´í„° ëª¨ë¸
class RouteQuery(BaseModel):
    """Route a user query to the most relevant datasource."""

    # ë°ì´í„° ì†ŒìŠ¤ ì„ íƒì„ ìœ„í•œ ë¦¬í„°ëŸ´ íƒ€ì… í•„ë“œ
    datasource: Literal["vectorstore", "web_search"] = Field(
        ...,
        description="Given a user question choose to route it to web search or a vectorstore.",
    )
# LLM ì´ˆê¸°í™” ë° í•¨ìˆ˜ í˜¸ì¶œì„ í†µí•œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
structured_llm_router = llm.with_structured_output(RouteQuery)
# prompt ìƒì„±
system = """You are an expert at routing a user question to a vectorstore or web search.
The vectorstore contains documents related to DEC 2023 AI Brief Report(SPRI) with OPEN AI, Domestic and foreign policy etc.
Use the vectorstore for questions on these topics. Otherwise, use web-search."""
# Routing ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
route_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ êµ¬ì¡°í™”ëœ LLM ë¼ìš°í„°ë¥¼ ê²°í•©í•˜ì—¬ ì§ˆë¬¸ ë¼ìš°í„° ìƒì„±
question_router = route_prompt | structured_llm_router
```
ìƒê¸°ì˜ question_routerì— AI Briefì™€ ì‚¼ì„± ê°€ìš°ìŠ¤ë‚˜, ì•¤íŠ¸ë¡œí”½ì— ëŒ€í•´ ì§ˆì˜ë¥¼ ë„£ì–´ì¤€ë‹¤ë©´, ê²°ê³¼ë¡œ vectorstoreë¼ëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ ì™¸ì— ê´€ë ¨ ì—†ëŠ” ê²ƒë“¤ì€ ëª¨ë‘ web_searchë¼ëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, ì–´ë””ë¡œ routingì„ í•´ì•¼í• ì§€ ê²°ì •ì„ í•´ì£¼ëŠ” ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.   

## Retreival Grader
ì´ì–´ì„œ Adaptive-RAGì—ì„œëŠ” **Query Routing**ì´ ì¤‘ìš”í•˜ë‹¤ê³  í–ˆìœ¼ë©°, ë˜í•œ, **Self-Correcting**ë„ ì¤‘ìš”í•˜ë‹¤ê³  í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¼ ì´ Self-Correctingì„ í•˜ê¸° ìœ„í•´ì„œëŠ” Relevence Checkë¥¼ ìˆ˜í–‰í•´ì•¼í•  ê²ë‹ˆë‹¤. ì¦‰, Retrieveí•œ ë¬¸ì„œì˜ ê²°ê³¼ê°€ ì •ë§ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì¤˜ì•¼í•©ë‹ˆë‹¤. ìµœì¢… LLMì˜ ë‹µë³€ì€ ì´ Retrieveí•œ ë¬¸ì„œì— ë”°ë¼ ë‹µë³€ì˜ ì •í™•ë„ê°€ í¬ê²Œ ë‹¬ë¼ì§€ê¸°ë•Œë¬¸ì…ë‹ˆë‹¤. ê·¸ëŸ¼ ì´ Retrieveí•œ ë¬¸ì„œì˜ ì§ˆì„ ë†’ì´ê¸° ìœ„í•´ Self-Correctingì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ í•˜ê¸°ì— êµ¬í˜„í•´ë³´ë„ë¡ í•´ë³´ê² ìŠµë‹ˆë‹¤.   
```python
# ë¬¸ì„œ í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„° ëª¨ë¸ ì •ì˜
class GradeDocuments(BaseModel):
    """Binary score for relevance check on retrieved documents."""

    binary_score: str = Field(
        description="Documents are relevant to the question, 'yes' or 'no'"
    )
# LLM ì´ˆê¸°í™” ë° í•¨ìˆ˜ í˜¸ì¶œì„ í†µí•œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
structured_llm_grader = llm.with_structured_output(GradeDocuments)
# ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì‚¬ìš©ì ì§ˆë¬¸ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
system = """You are a grader assessing relevance of a retrieved document to a user question. \n 
    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \n
    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n
    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."""
grade_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "Retrieved document: \n\n {document} \n\n User question: {question}"),
    ]
)
# ë¬¸ì„œ ê²€ìƒ‰ê²°ê³¼ í‰ê°€ê¸° ìƒì„±
retrieval_grader = grade_prompt | structured_llm_grader
```
ìƒê¸°ì˜ ì½”ë“œì— ëŒ€í•´ ì•½ê°„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. í•´ë‹¹ LLMì€ ë‹¨ìˆœ Relevence Checkë¥¼ í•˜ëŠ” ìš©ë„ì´ë©´ ë‹µë³€ ë˜í•œ, ê´€ë ¨ì„±ì´ ìˆë‹¤ë©´, yes ì—†ë‹¤ë©´ noë¥¼ ì¶œë ¥í•˜ê²Œë©ë‹ˆë‹¤. ì¦‰, ì§ˆë¬¸ê³¼ retrieveì˜ ê´€ë ¨ì„±ì´ ê²€ì‚¬í•©ë‹ˆë‹¤. ë§Œì•½ ì‚¬ìš©ìì˜ ì§ˆì˜ë¥¼ í†µí•´ retrieveí•œ ê²ƒì´ ì§ˆì˜ì™€ ê´€ë ¨ì´ ìˆë‹¤ë©´ yesë¥¼ ì—†ë‹¤ë©´ noë¥¼ ì¶œë ¥í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ë§Œì•½ yesë¼ë©´ ìµœì¢… ë‹µë³€ Nodeë¡œ, noë¼ë©´ Query Rewrite Nodeë¡œ ê°„ë‹¤ë©´, ë” ì •í™•í•œ ë‹µë³€ì„ ë‚´ë±‰ì„ ìˆ˜ ìˆê²Œ ë  ê²ƒ ì…ë‹ˆë‹¤.   

## RAG Chain and Hallucination Check
ì´ì–´ì„œ ë¹ ë¥´ê²Œ RAG Chain êµ¬ì„±í•˜ê³  ìµœì¢… ë‹µë³€ì— ëŒ€í•´ì„œë„ Relevence Check(Hallucination Check)í•˜ë„ë¡ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤. ìš°ì„  ë¨¼ì € RAG Chainì„ êµ¬í˜„í•˜ê² ìŠµë‹ˆë‹¤.   
```python
prompt = PromptTemplate(
    template="""You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. 
    If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
    Question: {question}
    Context: {context} 
    Answer:""",
    input_variables=["context", "question"],
)
# LLM ì´ˆê¸°í™”
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
# ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜
def format_docs(docs):
    return "\n\n".join(
        [
            f'<document><content>{doc.page_content}</content><source>{doc.metadata["source"]}</source><page>{doc.metadata["page"]+1}</page></document>'
            for doc in docs
        ]
    )
# RAG ì²´ì¸ ìƒì„±
rag_chain = prompt | llm | StrOutputParser()
```
RAG Chainì€ ë‹¤ë“¤ ì•Œ ê²ƒì´ë¼ ìƒê°í•´ì„œ ë¹ ë¥´ê²Œ ë„˜ì–´ê°”ìŠµë‹ˆë‹¤.    
ì´ì–´ì„œ ê·¸ëŸ¼ Hallucination Checkë¥¼ êµ¬í˜„í•´ë³´ê¸° ì•ì„œ ê°„ë‹¨í•˜ê²Œ ì™œ í•„ìš”í•œì§€ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œì˜ Relevence CheckëŠ” LLMì´ ìµœì¢… ë‹µë³€ì„ í•œ ê²ƒì´ ì •ë§ ì‚¬ìš©ìì˜ ì§ˆì˜ì™€ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ì „ì—ë„ Relevence Checkë¥¼ í•˜ê¸´ í•˜ì§€ë§Œ, ì´ ê²½ìš° Queryì˜ ì¬ì‘ì„±í•˜ë©° Retrieveì™€ Queryê°„ì˜ ê´€ë ¨ì„± ê²€ì‚¬ë§Œ í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ìš°ë¦¬ì˜ **ìµœì¢… ëª©í‘œëŠ” ì‚¬ìš©ì ì§ˆì˜ì™€ ê´€ë ¨ ìˆëŠ” ë‹µë³€ì„ ìƒì„±**í•˜ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ìƒì„±í•œ ë‹µë³€ì´ ì •ë§ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ì¸ì§€ë¥¼ í•œ ë²ˆë” ê²€ì‚¬ë¥¼ í•´ì¤€ë‹¤ë©´ ë” ì•ˆì „í•˜ë©° ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìˆì„ ê²ƒ ì…ë‹ˆë‹¤. ë°©ì‹ê³¼ êµ¬ì¡°ëŠ” ê¸°ì¡´ Relevence Checkì™€ ë§¤ìš° í¡ì‚¬í•©ë‹ˆë‹¤. Promptë§Œ ì˜ ì‘ì„±ì„ í•´ì£¼ë©´ ë©ë‹ˆë‹¤. í•˜ê¸°ì— êµ¬í˜„ì„ í•´ë³´ê² ìŠµë‹ˆë‹¤.   
```python
# í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬ë¥¼ ìœ„í•œ ë°ì´í„° ëª¨ë¸ ì •ì˜
class GradeHallucinations(BaseModel):
    """Binary score for hallucination present in generation answer."""

    binary_score: str = Field(
        description="Answer is grounded in the facts, 'yes' or 'no'"
    )
# í•¨ìˆ˜ í˜¸ì¶œì„ í†µí•œ LLM ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
structured_llm_grader = llm.with_structured_output(GradeHallucinations)
# í”„ë¡¬í”„íŠ¸ ì„¤ì •
system = """You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n 
    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts."""
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
hallucination_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "Set of facts: \n\n {documents} \n\n LLM generation: {generation}"),
    ]
)
# í™˜ê° í‰ê°€ê¸° ìƒì„±
hallucination_grader = hallucination_prompt | structured_llm_grader

class GradeAnswer(BaseModel):
    """Binary scoring to evaluate the appropriateness of answers to questions"""
    binary_score: str = Field(
        description="Indicate 'yes' or 'no' whether the answer solves the question"
    )
# í•¨ìˆ˜ í˜¸ì¶œì„ í†µí•œ LLM ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
structured_llm_grader = llm.with_structured_output(GradeAnswer)
# í”„ë¡¬í”„íŠ¸ ì„¤ì •
system = """You are a grader assessing whether an answer addresses / resolves a question \n 
     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question."""
answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "User question: \n\n {question} \n\n LLM generation: {generation}"),
    ]
)
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ êµ¬ì¡°í™”ëœ LLM í‰ê°€ê¸°ë¥¼ ê²°í•©í•˜ì—¬ ë‹µë³€ í‰ê°€ê¸° ìƒì„±
answer_grader = answer_prompt | structured_llm_grader
```

## Query Rewrite
ì´ì „ ì ˆì—ì„œ ë°°ì› ë˜ê²ƒê³¼ ë¹„ìŠ·í•˜ê²Œ Query Rewriteë¡œ êµ¬ì¡°í™”ëœ LLMì„ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.   
```python
# LLM ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
# Query Rewriter í”„ë¡¬í”„íŠ¸ ì •ì˜(ììœ ë¡­ê²Œ ìˆ˜ì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤)
system = """You a question re-writer that converts an input question to a better version that is optimized \n 
for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning."""
# Query Rewriter í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
re_write_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        (
            "human",
            "Here is the initial question: \n\n {question} \n Formulate an improved question.",
        ),
    ]
)
# Query Rewriter ìƒì„±
question_rewriter = re_write_prompt | llm | StrOutputParser()
```
## TavilySearch
TavilySearchë¥¼ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.
```python
from langchain_core.documents import Document

web_search_tool = TavilySearchResults(
    max_results=3
    )
search_result = web_search_tool.invoke("í…Œë””ë…¸íŠ¸ ìœ„í‚¤ë…ìŠ¤ ë­ì²´ì¸ íŠœí† ë¦¬ì–¼ URL ì„ ì•Œë ¤ì£¼ì„¸ìš”")
web_results_docs = [
    Document(
        page_content=web_result["content"],
        metadata={"source": web_result["url"]},
    )
    for web_result in search_result
]
print(web_results_docs)
```

## LangGraph Node Function
ì ì´ì œ í•„ìš”í•œ ì—­í• ì„ í•˜ëŠ” êµ¬ì¡°í™”ëœ LLMë“¤ì„ ëª¨ë‘ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¼ ì´ì œ LangGraphì˜ Nodeë¥¼ ì—­í• ì„ í•  ìˆ˜ ìˆë„ë¡ Nodeë¥¼ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.   
ìš°ì„  GraphStateë¶€í„° ì •ì˜í•´ë³´ê² ìŠµë‹ˆë‹¤. GraphStateëŠ” ì§ˆì˜ì¸ **question**, ë‹µë³€ì¸ **generation**, ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ì¸ **documents**ê°€ í•„ìš”í•©ë‹ˆë‹¤.   
```python
from typing import List
from typing_extensions import TypedDict, Annotated

# ê·¸ë˜í”„ì˜ ìƒíƒœ ì •ì˜
class GraphState(TypedDict):
    """
    ê·¸ë˜í”„ì˜ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„° ëª¨ë¸

    Attributes:
        question: ì§ˆë¬¸
        generation: LLM ìƒì„±ëœ ë‹µë³€
        documents: ë„íë¨¼í‹‘ ë¦¬ìŠ¤íŠ¸
    """
    question: Annotated[str, "User question"]
    generation: Annotated[str, "LLM generated answer"]
    documents: Annotated[List[str], "List of documents"]
``` 

ì°¨ê³¡ ì°¨ê³¡ í•˜ë‚˜ì”© êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤. ë¨¼ì € Adaptive-RAGì˜ FLOWë¥¼ ìƒê°í•´ë³´ë©´, **Query Routingì—ì„œ Web-Searchì™€ Retrieval Nodeë¡œ ë¶„ê¸°**ë©ë‹ˆë‹¤. ë¨¼ì € ì´ 2ê°€ì§€ë¥¼ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.   
```python
"""ì›¹ ê²€ìƒ‰ Node"""
def web_search(state: GraphState) -> GraphState:
    tool = TavilySearchResults(
        max_results=3
        )

    search_query = state["question"]
    search_result = tool.invoke(search_query)
    web_results_docs = [
        Document(
            page_content=web_result["content"],
            metadata={"source": web_result["url"]},
        )
        for web_result in search_result
    ]
    return GraphState(documents=web_results_docs, question=search_query)
"""Retrieve Node"""
def retrieve(state: GraphState) -> GraphState:
    print("==== [RETRIEVE] ====")
    question = state["question"]
    documents = pdf_retriever.invoke(question)
    return GraphState(documents=documents, question=question)
```

ì´ì–´ì„œ ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤. retrieveí•œ **ë¬¸ì„œì˜ ê²°ê³¼ì™€ ë‹µë³€ê°„ì˜ ê´€ë ¨ì„±ì„ í‰ê°€**í•œ í›„, ê´€ë ¨ì„±ì´ ì—†ë‹¤ë©´, **Queryë¥¼ ì¬ì‘ì„±**ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ë²ˆì—ëŠ” ì´ 2ê°€ì§€ë¥¼ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.    
```python
"""ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€"""
def grade_documents(state: GraphState) -> GraphState:
    print("==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====")
    question = state["question"]
    documents = state["documents"]
    
    # ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œë§Œ ë‚¨ê¸°ê¸°
    relevence_documents = []
    for doc in documents:
        grade = retrieval_grader.invoke({"question": question, "document": doc})
        score = grade.binary_score
        # ê´€ë ¨ì„±ì´ ìˆë‹¤ë©´ ì¶”ê°€
        if score == "yes":
            print("---GRADE: DOCUMENT RELEVANT---")
            relevence_documents.append(doc)
        else:
            print("---GRADE: DOCUMENT NOT RELEVANT---")
            continue
    return GraphState(question=question, documents=relevence_documents)
"""ì¿¼ë¦¬ ì¬ì‘ì„±"""
def rewrite_query(state: GraphState) -> GraphState:
    print("==== [TRANSFORM QUERY] ====")
    question = state["question"]
    documents = state["documents"]
    rewrite_query = question_rewriter.invoke({"question":question})
    return GraphState(question=rewrite_query, documents=documents)
```

ìƒê¸°ì˜ ì½”ë“œëŠ” retrieveí•œ ë¬¸ì„œì— ëŒ€í•´ ì•„ê¹Œ êµ¬í˜„í•œ retrieval_grader LLMì„ í†µí•´ ê´€ë ¨ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤. ë§Œì•½ queryì™€ ê´€ë ¨ì„±ì´ ìˆëŠ” documentë¼ë©´ ë‚¨ê¸°ê³  ì—†ë‹¤ë©´ documentë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.     
ì´ì–´ì„œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” generate Nodeë¥¼ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.   
```python
def generate(state: GraphState) -> GraphState:
    print("==== [GENERATE] ====")
    # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    question = state["question"]
    documents = state["documents"]

    # RAG ë‹µë³€ ìƒì„±
    generation = rag_chain.invoke({"context": documents, "question": question})
    return GraphState(question=question, documents=documents, generation=generation)
```

## LangGraph Edge
ì´ì œ Nodeë“¤ì„ ëª¨ë‘ ìƒì„±í–ˆìœ¼ë‹ˆ, self-Correctingì„ í•˜ë„ë¡ ìˆ˜í–‰í•˜ëŠ” Edgeë“¤ì„ êµ¬í˜„í•´ì•¼í•˜ëŠ” ì°¨ë¡€ì…ë‹ˆë‹¤. ê³„ì†í•´ì„œ Adaptive-RAGì˜ FLOWë¥¼ ìŠì§€ ì•Šì•„ì•¼í•©ë‹ˆë‹¤. ê°€ì¥ ë¨¼ì € **ì‚¬ìš©ì ì§ˆì˜ì— ë”°ë¼ ì–´ë–¤ Nodeë¡œ ë¶„ê¸°í• ì§€ë¥¼ ê²°ì •í•˜ëŠ” Query Routing**ì´ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ í•˜ê¸°ì— êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.   
```python
def query_routing(state: GraphState) -> Literal["web_search", "vectorstore"]:
    print("==== [ROUTE QUESTION] ====")
    # ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
    question = state["question"]
    router_result = question_router.invoke({"question": question})
    if router_result == "web_serach":
        print("==== [ROUTE QUESTION TO WEB SEARCH] ====")
        return "web_search"
    else:
        print("==== [ROUTE QUESTION TO VECTORSTORE] ====")
        return "vectorstore"
``` 
ìƒê¸°ì˜ ì½”ë“œë¥¼ ë³´ë©´, ì´ì „ì— ë§Œë“¤ì—ˆë˜ question_routerì˜ LLMì„ í†µí•´ í•´ë‹¹ ì§ˆì˜ê°€ AI BREIFì™€ ê´€ë ¨ì´ ìˆë‹¤ë©´ "vectorstore", ì—†ë‹¤ë©´ "web_search"ë¥¼ ë‹µë³€í•˜ê²Œë©ë‹ˆë‹¤. ì´ì— ë”°ë¼ ì–´ë–¤ Nodeë¡œ ë¶„ê¸°í• ì§€ë¥¼ ê²°ì •í•´ì¤ë‹ˆë‹¤.    
ê·¸ëŸ¼ ì´ì–´ì„œ ë˜ Adaptive-RAGì—ì„œ ì–´ë–¤ ë¶„ê¸°ì ì´ ì¡´ì¬í•˜ëŠ”ì§€ ìƒê°í•´ë´…ì‹œë‹¤. **retrieveí•œ ê²°ê³¼ê°€ Queryì™€ ê´€ë ¨ì„±ì´ ì—†ë‹¤ë©´ Query Rewrite Node**ë¡œ **ê´€ë ¨ì„±ì´ ìˆë‹¤ë©´ ë°”ë¡œ Answer Nodeë¡œ ë¶„ê¸°**í•˜ëŠ” ì ì´ ì¡´ì¬í•´ì•¼í•©ë‹ˆë‹¤. ê·¸ëŸ¼ retrieveí•œ ë¬¸ì„œì™€ queryê°„ì˜ ê´€ë ¨ì„± ê²€ì‚¬ë¥¼ ìˆ˜í–‰ í›„, ë¶„ê¸°ë¥¼ ì‹œì¼œì£¼ë©´ ë  ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ í•˜ê¸°ì— êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.   
```python
def check_retrieve_doc(state: GraphState) -> Literal["query_rewrite", "generate"]:
    print("==== [DECISION TO GENERATE] ====")
    # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    question = state["question"]
    filtered_documents = state["documents"]
    # documentê°€ ì—†ë‹¤ë©´ query ì¬ì‘ì„±
    if not filtered_documents or len(filtered_documents) == 0:
        print("==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUERY] ====")
        return "query_rewrite"
    else:
        print("==== [DECISION: GENERATE] ====")
        return "generate"
```

ìƒê¸°ì˜ ì½”ë“œë¥¼ í™•ì¸í•´ë³´ë©´ retrieve Nodeì—ì„œ Relevence Checkë¥¼ ìˆ˜í–‰ í›„, Relevenceí•œ documentë“¤ì´ ìˆë‹¤ë©´, ê·¸ëŸ° documentë“¤ë§Œ ë‚¨ê²¨ë†“ê³  ë‚˜ë¨¸ì§„ ì‚­ì œë¥¼ í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ëª¨ë‘ ì‚­ì œê°€ ë˜ì–´ documentê°€ ì—†ë‹¤ë©´ query rewrite Nodeë¡œ ë¶„ê¸°ë¥¼ í•˜ë©´ë˜ê³ , documentê°€ 1ê°œë¼ë„ ì¡´ì¬í•œë‹¤ë©´ ê·¸ê±¸ ê¸°ë°˜ìœ¼ë¡œ LLMì´ ìµœì¢… ë‹µë³€ì„ ìƒì„±ë˜ê²Œ ì§„í–‰í•˜ë©´ ë  ê²ƒ ì…ë‹ˆë‹¤.   
ì´ì–´ì„œ ë§ˆì§€ë§‰ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì´ Queryì˜ ì˜ë„ì— ë§ê²Œ ì˜ ìƒì„±ë˜ì—ˆëŠ”ì§€ë¥¼ í™•ì¸ì„ í•´ì¤˜ì•¼í•©ë‹ˆë‹¤. ì¦‰, **Hallucinationì´ ë°œìƒí–ˆë‹¤ë©´, ë‹¤ì‹œ í•œë²ˆ ë” ë‹µë³€ì„ ì¬ìƒì„±í•´ì£¼ê³ (generate Nodeë¡œ)**, Halluciniationì€ ì•„ë‹ˆì§€ë§Œ, **ë‹µë³€ì´ Queryì™€ ì—°ê´€ì„±ì´ ì—†ë‹¤ë©´, ë‹¤ì‹œ Query Rewriteë¡œ**, ê·¸ê²Œ ì•„ë‹ˆë¼ë©´ ìµœì¢…ì ìœ¼ë¡œ ë‹µë³€ì„ ì˜ ìƒì„±í–ˆìœ¼ë‹ˆ ì¢…ë£Œë¥¼ í•˜ê²Œí•˜ë©´ ë©ë‹ˆë‹¤. ì´ë¥¼ í•˜ê¸°ì— êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.   
```python
def hallucination_check(state: GraphState) -> Literal["hallucination", "not_relevent", "relevant"]:
    print("==== [CHECK HALLUCINATIONS] ====")
    # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    question = state["question"]
    documents = state["documents"]
    generation = state["generation"]
    hallu_evaluator = hallucination_grader.invoke({"documents": docs, "generation": generation})
    score = hallu_evaluator.binary_score
    # í• ë£¨ì‹œë„¤ì´ì…˜ì´ ì—†ë‹¤ë©´, 
    if score == "yes":
        print("==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====")
        print("==== [GRADE GENERATED ANSWER vs QUESTION] ====")
        answer_evaluator = answer_grader.invoke({"question": question, "generation": generation})
        score = answer_evaluator.binary_score
        # ë‹µë³€ì´ Queryì™€ ì˜ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ í™•ì¸
        if score == "yes":
            print("==== [DECISION: GENERATED ANSWER ADDRESSES QUESTION] ====")
            return "relevant"
        else:
            print("==== [DECISION: GENERATED ANSWER DOES NOT ADDRESS QUESTION] ====")
            return "not_relevent"
    # í• ë£¨ë„¤ì´ì…˜ ë°œìƒ -> ë‹µë³€ ì¬ìƒì„±
    else:
        print("==== [DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY] ====")
        return "hallucination"
```

ìƒê¸°ì˜ ì½”ë“œëŠ” ì´ì „ì— ë§Œë“¤ì—ˆë˜ hallucination evalutorì™€, ì •ë‹µê³¼ queryê°„ì˜ ê´€ë ¨ì„± í‰ê°€ë¥¼ í•˜ëŠ” answer evaluatorë¥¼ ëª¨ë‘ ì´ìš©í•˜ì—¬ í‰ê°€í•˜ì—¬ ë¶„ê¸°í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ìš°ì„  í• ë£¨ì‹œë„¤ì´ì…˜ì´ ìˆëŠ”ì§€ hallucination evalutorë¥¼ í†µí•´ í‰ê°€í•´ì„œ **í• ë£¨ì‹œë„¤ì´ì…˜ì´ ìˆë‹¤ë©´ ë‹µë³€ ì¬ìƒì„±**, í• ë£¨ì‹œë„¤ì´ì…˜ì´ ì—†ë‹¤ë©´, ë‹µë³€ê³¼ Queryê°„ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤. **Queryì™€ ê´€ë ¨ì„±ì´ ìˆë‹¤ë©´ ìµœì¢… ë‹µë³€ ìƒì„±**, ì—†ë‹¤ë©´ **Queryë¥¼ ì¬ì‘ì„±í•˜ëŠ” Nodeë¡œ ë¶„ê¸°**í•©ë‹ˆë‹¤.   
## Graph Compile
ì´ì œ ìƒê¸°ì˜ êµ¬í˜„í•œ Nodeì™€ Edgeí•¨ìˆ˜ë“¤ì„ Graphë¡œ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤. ê³„ì†í•˜ì—¬ Adaptive-RAGì˜ FLOWë¥¼ ìƒê°í•˜ë©° ìµœì¢… Graphì˜ ë…¸ë“œë“¤ì„ branchë¡œ ì˜ ì´ì–´ì¤˜ì•¼í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ëœ Graphì˜ êµ¬ì¡°ë¥¼ ì´ë¯¸ì§€ë¡œ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.   
```python
from langgraph.graph import END, StateGraph, START
from langgraph.checkpoint.memory import MemorySaver

# ê·¸ë˜í”„ ìƒíƒœ ì´ˆê¸°í™”
workflow = StateGraph(GraphState)

# ë…¸ë“œ ì •ì˜
workflow.add_node("retrieve", retrieve)
workflow.add_node("web_search", web_search)
workflow.add_node("query_rewrite", rewrite_query)
workflow.add_node("retrieve_evaluator", grade_documents)
workflow.add_node("generate", generate)

# ì—£ì§€ ì •ì˜ 
workflow.add_conditional_edges(
    START,
    query_routing, # query routing -> [web_search, retrieve]
    {
        "web_search": "web_search",
        "vectorstore": "retrieve"
    }
)
workflow.add_edge("web_search", "generate") # web serach -> generate
workflow.add_edge("retrieve", "retrieve_evaluator") # retrieve -> grade_documents
workflow.add_conditional_edges(
    "retrieve_evaluator",
    check_retrieve_doc, # grade_documents -> [query_rewrite, generate]
    {
        "query_rewrite": "query_rewrite",
        "generate": "generate"
    }
)
workflow.add_edge("query_rewrite", "retrieve") # query_rewrite -> retrieve
workflow.add_conditional_edges(
    "generate",
    hallucination_check, # generate -> [query_rewrite, END, generate]
    {
        "hallucination": "generate",
        "not_relevent": "query_rewrite",
        "relevant": END
    }
)

app = workflow.compile(checkpointer=MemorySaver())
```

<div style="text-align : center;">
<img src="../../../assets/images/LangChain/2025-03-22-langgraph12/adpative_rag_langgraph.jpeg" alt="adpative_rag_langgraph" style="zoom:150%;" />    
</div>    


```python
from langchain_core.runnables import RunnableConfig
from stream import *

# stream function

inputs = GraphState(question="AIê´€ë ¨ ì •ì±…")
node_names = ["web_search" ,"retrieve", "retrieve_evaluator", "query_rewrite", "generate"]
# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)
config = RunnableConfig(recursion_limit=20, configurable={"thread_id": "1"})

streamer(app=app, inputs=inputs, config=config, node_names=node_names)
```

```bash
==== [ROUTE QUESTION] ====
==== [ROUTE QUESTION TO VECTORSTORE] ====
==== [RETRIEVE] ====
==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====

==================================================
ğŸ”„ Node: retrieve_evaluator ğŸ”„
- - - - - - - - - - - - - - - - - - - - - - - - - 
---GRADE: DOCUMENT RELEVANT---
---GRADE: DOCUMENT RELEVANT---
---GRADE: DOCUMENT RELEVANT---
---GRADE: DOCUMENT RELEVANT---
==== [DECISION TO GENERATE] ====
==== [DECISION: GENERATE] ====
==== [GENERATE] ====

==================================================
ğŸ”„ Node: generate ğŸ”„
- - - - - - - - - - - - - - - - - - - - - - - - - 
AI ê´€ë ¨ ì •ì±…ìœ¼ë¡œëŠ” ì¤‘êµ­ì˜ ë”¥í˜ì´í¬ ê²Œì‹œ ê¸ˆì§€ ê·œì •ê³¼ ì˜êµ­ì˜ íƒˆíƒ„ì†Œë¥¼ ìœ„í•œ AI í”„ë¡œê·¸ë¨ì´ ìˆìŠµë‹ˆë‹¤. ë¯¸êµ­ì€ AIì— ëŒ€í•œ ë¯¼ê°„ ê¸°ì—…ì˜ ììœ¨ ê·œì¹™ì„ ì œì•ˆí•˜ëŠ” 'AI Bill of Rights'ë¥¼ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì •ì±…ë“¤ì€ AI ê¸°ìˆ ì˜ ë°œì „ê³¼ ì‚¬íšŒì  ì±…ì„ì„ ë™ì‹œì— ê³ ë ¤í•˜ê³  ìˆìŠµë‹ˆë‹¤.==== [CHECK HALLUCINATIONS] ====
==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====
==== [GRADE GENERATED ANSWER vs QUESTION] ====
==== [DECISION: GENERATED ANSWER ADDRESSES QUESTION] ====
```
ìƒê¸°ì˜ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.