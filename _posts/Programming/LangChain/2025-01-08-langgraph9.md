---
title:  "LangGraph(RAG)2"
categories: LangChain
tag: [theory, AI, LLM, LangChain, LangGraph, RAG]
toc: true
author_profile: false
sidebar:
    nav: "docs"
use_math: true
excerpt: Prompt
comments: true
date: 2025-01-08
toc_sticky: true
---
하기의 내용은 <a href="https://wikidocs.net/233801" target="_blank">LangChain 노트</a> 기반으로 작성했습니다.

# Evaluator
항상 LLM이 정확한 답변을 하는지 신뢰가 부족할 수 있습니다. RAG구조에서 답변이 잘못된 경우에 고려해볼 수 있는 경우는 크게 2가지가 존재한다고 볼 수 있습니다.   
① Retrieval의 결과가 이상한 경우    
② LLM이 답변을 질문과 무관하게 내뱉은 경우    
그럼 이를 한 번 평가하는 코드에 대해 알아보겠습니다. 이 평가자인 Evaluator는 직접 Custom으로 만들 수도 있고, API에서 제공해주기도 합니다. 이번절에서는 ②인 LLM 답변이 질문과 무관하게 내뱉었는지에 대한 평가를 해보겠습니다. 이 평가자로 LangSmith에서 제공해주는 LangChainStringEvaluator을 이용하여 평가해보겠습니다.   

```python
class RAGChain:
    def __init__(self, pdf, llm):
        self.llm = llm
        self.pdf = pdf
    
    def create_retriever(self, 
                         embeddings: object = OpenAIEmbeddings(model="text-embedding-ada-002")):
        loader = PyPDFLoader(self.pdf)

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        split_docs = loader.load_and_split(text_splitter)
        vector = FAISS.from_documents(split_docs, embeddings)
        retriever = vector.as_retriever()
        return retriever

        
    def format_docs(self, docs):
        """검색된 문서들을 하나의 문자열로 포맷팅"""
        context = ""
        for doc in docs:
            context += doc.page_content
            context += '\n'
        return context
            

    def invoke(self, inputs):
        question = inputs.get("question", "")
        context = inputs.get("context", [])
        if isinstance(context, list):
            context = self.format_docs(context)
        history = inputs.get("chat_history", [])
        history = " ".join(history)
        
        template = """
        다음 정보는 이전 대화에 대한 내용입니다.
        {chat_history}
        
        다음 정보를 바탕으로 질문에 답하세요:
        {context}

        질문: {question}
        
        주어진 질문에만 답변하세요. 문장으로 답변해주세요.
        답변:
        """
        prompt = PromptTemplate.from_template(template)
        
        rag_chain = (
            prompt
            | self.llm
            | StrOutputParser()
        )
        answer = rag_chain.invoke({"chat_history": history, "context":context, "question":question })
        
        return answer

def ask_llm(llm):
    rag_chain = RAGChain(llm=llm, pdf="./data/AI_brief_2023년_2월호.pdf")
    retriever = rag_chain.create_retriever()
    
    def _ask_question(inputs):
        context = retriever.invoke(inputs["question"])
        context = "\n".join([doc.page_content for doc in context])
        return {
            "question": inputs["question"],
            "context": context,
            "answer": rag_chain.invoke(inputs["question"]),
        }
    return _ask_question


gpt_chain = ask_llm(llm=ChatOpenAI(model="gpt-4o-mini"))
from langsmith.evaluation import evaluate, LangChainStringEvaluator
cot_qa_evalulator = LangChainStringEvaluator(
    "cot_qa",
    config={"llm": ChatOpenAI(model="gpt-4o-mini", temperature=0)},
    prepare_data=lambda run, example: {
        "prediction": run.outputs["answer"],
        "reference": run.outputs["context"],
        "input": example.inputs["question"],
    },
)

dataset_name = "RAG_EVAL_DATASET"
experiment_results1 = evaluate(
    gpt_chain,
    data=dataset_name,
    evaluators=[cot_qa_evalulator],
    experiment_prefix="MODEL_COMPARE_EVAL",
    # 실험 메타데이터 지정
    metadata={
        "variant": "GPT-4o-mini 평가 (cot_qa)",
    },
)
```

<br>
이어서 이번에는 ①인 Retrieval가 질문에 알맞은 Context를 잘 가져왔는지를 평가해보도록 하겠습니다. 

# Advanced Naive RAG(Relevance Node)
이번 절에서는 이전 절에 이어 Naive RAG 구조에서 조금 더 정확한 답변을 할 수 있도록 Retrieval에 대한 Relevance Check를 하는 Node를 추가해보도록 하겠습니다. 이전 절에서의 코드와 동일하며 Relevance Check Node가 추가된 형태입니다.   
그럼 Relevance Node를 구현해보겠습니다.   

